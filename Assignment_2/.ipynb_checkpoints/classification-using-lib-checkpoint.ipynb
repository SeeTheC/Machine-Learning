{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas;\n",
    "import seaborn as sb;\n",
    "import numpy as np;\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "def readCSVFile(file):\n",
    "    data=pandas.read_csv(file,\",\",header=0, na_values='?', skipinitialspace=True);\n",
    "    return data;\n",
    "    pass;\n",
    "def readTrainData(dataset):    \n",
    "    return dataset.ix[:,1:-1], dataset.ix[:,-1:];\n",
    "    pass;\n",
    "\n",
    "def readTestData(dataset):    \n",
    "    return dataset.ix[:,1:],dataset.ix[:,0:1];\n",
    "    pass;\n",
    "\n",
    "def normalizePhi(unNormalizedPhi,last_col_bias=False):    \n",
    "    #assuming last column as bias column\n",
    "    no_of_column=len(unNormalizedPhi[0]);\n",
    "    phi=np.array(unNormalizedPhi);\n",
    "    print(\"Normalizing Phi...\");  \n",
    "    std=phi.std(0);\n",
    "    mean=phi.mean(0);    \n",
    "    std[no_of_column-1]=1;\n",
    "    mean[no_of_column-1]=0;\n",
    "    #phi_normalize=(phi-mean)/std;    \n",
    "    \n",
    "    max_vector=phi.max(axis=0)\n",
    "    phi_normalize=phi/max_vector;    \n",
    "    \n",
    "    print(\"Normalization done.\");\n",
    "    return phi_normalize;\n",
    "    pass;\n",
    "\n",
    "def writeTestData(test_id,ystar,filenumber=0):    \n",
    "    fo = open(\"sampleSubmission-\"+str(filenumber)+\".csv\", \"w\");               \n",
    "    fo.write(\"id,salary\\n\");\n",
    "    m=len(ystar);\n",
    "    for i in range(m):\n",
    "        fo.write(str(test_id[i][0])+\",\"+str(ystar[i][0])+\"\\n\");\n",
    "    fo.close();\n",
    "    pass;\n",
    "\n",
    "def dropColumns(dataframe,colList):\n",
    "    for c in colList:\n",
    "        dataframe.drop([c], axis = 1, inplace = True);\n",
    "    pass;\n",
    "\n",
    "def addColByCategory(dataset):\n",
    "    return pandas.get_dummies(dataset);\n",
    "    pass;\n",
    "\n",
    "def categoryToNumber(dataset,categoryList):\n",
    "    for c in categoryList:\n",
    "        if (c in dataset):            \n",
    "            dataset[c]=pandas.get_dummies(dataset[c]).values.argmax(1);        \n",
    "    return dataset;\n",
    "    pass;\n",
    "    \n",
    "\n",
    "def handleCategoryData(dataset,categoryList=None,byNumber=False):\n",
    "    if(byNumber):\n",
    "        return categoryToNumber(dataset,categoryList)\n",
    "    else:\n",
    "        return addColByCategory(dataset);\n",
    "def findMostFrequentCount(dataset):\n",
    "    #arr=dataset;\n",
    "    #axis = 0\n",
    "    #u, indices = np.unique(arr, return_inverse=True)\n",
    "    #print(u);\n",
    "    #u[np.argmax(np.apply_along_axis(np.bincount, axis, indices.reshape(arr.shape),None, np.max(indices) + 1), axis=axis)]\n",
    "    x = np.array([0, 1, 1, 3, 2, 1, 2, 3]);\n",
    "    w=np.bincount(dataset[:,1].astype(int))#work-class: Private\n",
    "    o=np.bincount(dataset[:,6].astype(int))#occupation: Married-civ-spouse\n",
    "    c=np.bincount(dataset[:,13].astype(int))#country : US\n",
    "    pass;\n",
    "\n",
    "def fillNanValue(dataframe,col,value):\n",
    "    if (col in dataframe):\n",
    "        dataframe[col].fillna(value, inplace=True);\n",
    "    pass;\n",
    "\n",
    "def imputeUnknowValue(dataframe):\n",
    "    #by most frequent value;\n",
    "    fillNanValue(dataframe,\"workclass\",\"Private\");\n",
    "    fillNanValue(dataframe,\"occupation\",\"Craft-repair\");\n",
    "    fillNanValue(dataframe,\"native-country\",\"United-States\");\n",
    "    pass;\n",
    "\n",
    "def addRemainingCol(colList,dataframe,rowCount):\n",
    "    i=0;\n",
    "    for c in colList:\n",
    "        if( c not in dataframe):\n",
    "            dataframe.insert(i, c, 0);\n",
    "        i+=1;\n",
    "    pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--settings--\n",
    "pandas.set_option('display.max_columns', None);\n",
    "#---init---\n",
    "dir=\"data/\"\n",
    "trainFile=dir+\"train.csv\";\n",
    "testFile=dir+\"kaggle_test_data.csv\";\n",
    "categoryList=[\"workclass\",\"education\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"native-country\"];\n",
    "drop_col=['native-country',\"race\",\"education\"]\n",
    "trained_dataset=readCSVFile(trainFile);\n",
    "trained_data,trained_y=readTrainData(trained_dataset);\n",
    "\n",
    "test_dataset=readCSVFile(testFile);\n",
    "test_data,test_id=readTestData(test_dataset);\n",
    "\n",
    "#droping unrelated-columns\n",
    "dropColumns(trained_data,drop_col);\n",
    "dropColumns(test_data,drop_col);\n",
    "\n",
    "#impute:\n",
    "imputeUnknowValue(trained_data);\n",
    "imputeUnknowValue(test_data);\n",
    "\n",
    "#converting categorical data to point wise data\n",
    "byNumber=False;\n",
    "dummy_trained_data=handleCategoryData(trained_data,categoryList,byNumber);\n",
    "dummy_test_data=handleCategoryData(test_data,categoryList,byNumber);\n",
    "\n",
    "\n",
    "#adding missing column\n",
    "trained_columns_name=list(dummy_trained_data.columns.values);\n",
    "addRemainingCol(trained_columns_name,dummy_test_data,len(trained_data))\n",
    "test_columns_name=list(dummy_test_data.columns.values);\n",
    "\n",
    "#converting panda data frame to numpy martix\n",
    "mtx_dummy_tds=dummy_trained_data.as_matrix(columns=None)\n",
    "mtx_dummy_testds=dummy_test_data.as_matrix(columns=None)\n",
    "mtx_trained_y=trained_y.as_matrix(columns=None);\n",
    "mtx_test_id=test_id.as_matrix(columns=None);\n",
    "\n",
    "#adding bias column\n",
    "mtx_dummy_tds=np.column_stack((mtx_dummy_tds,np.ones((len(mtx_dummy_tds),1))))\n",
    "mtx_dummy_testds=np.column_stack((mtx_dummy_testds,np.ones((len(mtx_dummy_testds),1))))\n",
    "\n",
    "#normalization\n",
    "mtx_dummy_tds_norm=normalizePhi(mtx_dummy_tds)\n",
    "mtx_dummy_testds_norm=normalizePhi(mtx_dummy_testds)\n",
    "\n",
    "\n",
    "#print(mtx_dummy_tds)\n",
    "#print(mtx_dummy_tds_norm)\n",
    "#pandas.get_dummies(trained_data.ix[:,1:2])\n",
    "print(\"train\",np.shape(mtx_dummy_tds_norm),\"test\",np.shape(mtx_dummy_testds_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.ravel(trained_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "X = dummy_trained_data\n",
    "y = np.ravel(trained_y)\n",
    "\n",
    "index = [\"Decision Tree\", \"Random Forest\", \"K-Neighbors\", \"Gradient Boosting\", \"My Model\",\n",
    "         \"Logistic Regression\", \"Support Vector\", \"Multinomial NB\", \"Bernoulli NB\", \"Gaussian NB\"]\n",
    "\n",
    "columns = [\"Misclassified Points\", \"Accuracy\", \"AUC\"]\n",
    "modelComparision = pandas.DataFrame(index=index, columns=columns)\n",
    "\n",
    "dtc = 0, DecisionTreeClassifier()\n",
    "rfc = 1, RandomForestClassifier()\n",
    "knn = 2, KNeighborsClassifier()\n",
    "gbc = 3, GradientBoostingClassifier()\n",
    "lgr = 5, LogisticRegression()\n",
    "svc = 6, SVC()\n",
    "mnb = 7, MultinomialNB()\n",
    "bnb = 8, BernoulliNB()\n",
    "gnb = 9, GaussianNB()\n",
    "\n",
    "for i in lgr:\n",
    "    y_pred = i[1].fit(X, y).predict(X)\n",
    "    misclassifedPoints = (y_pred != y).sum()\n",
    "    accuracy = (len(dummy_trained_data.index) - misclassifedPoints) / len(dummy_trained_data.index)\n",
    "    AUC = roc_auc_score(y_pred, y)\n",
    "    modelComparision.ix[i[0]] = [misclassifedPoints, accuracy, AUC]\n",
    "\n",
    "modelComparision"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
