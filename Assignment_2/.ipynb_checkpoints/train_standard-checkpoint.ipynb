{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas;\n",
    "import numpy as np;\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "def readCSVFile(file):\n",
    "    data=pandas.read_csv(file,\",\",header=0, na_values='?', skipinitialspace=True);\n",
    "    return data;\n",
    "    pass;\n",
    "def readTrainData(dataset):    \n",
    "    return dataset.ix[:,1:-1], dataset.ix[:,-1:];\n",
    "    pass;\n",
    "\n",
    "def readTestData(dataset):    \n",
    "    return dataset.ix[:,1:],dataset.ix[:,0:1];\n",
    "    pass;\n",
    "\n",
    "def normalizePhi(unNormalizedPhi,last_col_bias=False):    \n",
    "    #assuming last column as bias column\n",
    "    no_of_column=len(unNormalizedPhi[0]);\n",
    "    phi=np.array(unNormalizedPhi);\n",
    "    std=phi.std(0);\n",
    "    mean=phi.mean(0);    \n",
    "    std[no_of_column-1]=1;\n",
    "    mean[no_of_column-1]=0;\n",
    "    #phi_normalize=(phi-mean)/std;    \n",
    "    \n",
    "    max_vector=phi.max(axis=0)\n",
    "    phi_normalize=phi/max_vector;    \n",
    "    \n",
    "    return phi_normalize;\n",
    "    pass;\n",
    "\n",
    "def writeTestData(test_id,ystar,filenumber=0,filename=None):\n",
    "    if(filename==None):\n",
    "        fo = open(\"log/output/sampleSubmission-\"+str(filenumber)+\".csv\", \"w\");               \n",
    "    else:\n",
    "        fo = open(filename+\".csv\", \"w\");        \n",
    "    fo.write(\"id,salary\\n\");\n",
    "    m=len(ystar);\n",
    "    for i in range(m):\n",
    "        fo.write(str(test_id[i][0])+\",\"+str(ystar[i])+\"\\n\");\n",
    "    fo.close();\n",
    "    pass;\n",
    "\n",
    "def dropColumns(dataframe,colList):\n",
    "    for c in colList:\n",
    "        dataframe.drop([c], axis = 1, inplace = True);\n",
    "    pass;\n",
    "\n",
    "def addColByCategory(dataset):\n",
    "    return pandas.get_dummies(dataset);\n",
    "    pass;\n",
    "\n",
    "def categoryToNumber(dataset,categoryList):\n",
    "    for c in categoryList:\n",
    "        if (c in dataset):            \n",
    "            dataset[c]=pandas.get_dummies(dataset[c]).values.argmax(1);        \n",
    "    return dataset;\n",
    "    pass;\n",
    "    \n",
    "\n",
    "def handleCategoryData(dataset,categoryList=None,byNumber=False):\n",
    "    if(byNumber):\n",
    "        return categoryToNumber(dataset,categoryList)\n",
    "    else:\n",
    "        return addColByCategory(dataset);\n",
    "def findMostFrequentCount(dataset):\n",
    "    #arr=dataset;\n",
    "    #axis = 0\n",
    "    #u, indices = np.unique(arr, return_inverse=True)\n",
    "    #print(u);\n",
    "    #u[np.argmax(np.apply_along_axis(np.bincount, axis, indices.reshape(arr.shape),None, np.max(indices) + 1), axis=axis)]\n",
    "    x = np.array([0, 1, 1, 3, 2, 1, 2, 3]);\n",
    "    w=np.bincount(dataset[:,1].astype(int))#work-class: Private\n",
    "    o=np.bincount(dataset[:,6].astype(int))#occupation: Married-civ-spouse\n",
    "    c=np.bincount(dataset[:,13].astype(int))#country : US\n",
    "    pass;\n",
    "\n",
    "def fillNanValue(dataframe,col,value):\n",
    "    if (col in dataframe):\n",
    "        dataframe[col].fillna(value, inplace=True);\n",
    "    pass;\n",
    "\n",
    "def imputeUnknowValue(dataframe):\n",
    "    #by most frequent value;\n",
    "    fillNanValue(dataframe,\"workclass\",\"Private\");\n",
    "    fillNanValue(dataframe,\"occupation\",\"Craft-repair\");\n",
    "    fillNanValue(dataframe,\"native-country\",\"United-States\");\n",
    "    pass;\n",
    "\n",
    "def addRemainingCol(colList,dataframe,rowCount):\n",
    "    i=0;\n",
    "    for c in colList:\n",
    "        if( c not in dataframe):\n",
    "            dataframe.insert(i, c, 0);\n",
    "        i+=1;\n",
    "    pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--settings--\n",
    "pandas.set_option('display.max_columns', None);\n",
    "#---init---\n",
    "dir=\"\"\n",
    "trainFile=dir+\"train.csv\";\n",
    "testFile=dir+\"kaggle_test_data.csv\";\n",
    "categoryList=[\"workclass\",\"education\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"native-country\"];\n",
    "drop_col=['native-country',\"race\",\"education\"]\n",
    "trained_dataset=readCSVFile(trainFile);\n",
    "trained_data,trained_y=readTrainData(trained_dataset);\n",
    "\n",
    "test_dataset=readCSVFile(testFile);\n",
    "test_data,test_id=readTestData(test_dataset);\n",
    "\n",
    "#droping unrelated-columns\n",
    "dropColumns(trained_data,drop_col);\n",
    "dropColumns(test_data,drop_col);\n",
    "\n",
    "#impute:\n",
    "imputeUnknowValue(trained_data);\n",
    "imputeUnknowValue(test_data);\n",
    "\n",
    "#converting categorical data to point wise data\n",
    "byNumber=False;\n",
    "dummy_trained_data=handleCategoryData(trained_data,categoryList,byNumber);\n",
    "dummy_test_data=handleCategoryData(test_data,categoryList,byNumber);\n",
    "\n",
    "\n",
    "#adding missing column\n",
    "trained_columns_name=list(dummy_trained_data.columns.values);\n",
    "addRemainingCol(trained_columns_name,dummy_test_data,len(trained_data))\n",
    "test_columns_name=list(dummy_test_data.columns.values);\n",
    "\n",
    "#converting panda data frame to numpy martix\n",
    "mtx_dummy_tds=dummy_trained_data.as_matrix(columns=None)\n",
    "mtx_dummy_testds=dummy_test_data.as_matrix(columns=None)\n",
    "mtx_trained_y=trained_y.as_matrix(columns=None);\n",
    "mtx_test_id=test_id.as_matrix(columns=None);\n",
    "\n",
    "#adding bias column\n",
    "mtx_dummy_tds=np.column_stack((mtx_dummy_tds,np.ones((len(mtx_dummy_tds),1))))\n",
    "mtx_dummy_testds=np.column_stack((mtx_dummy_testds,np.ones((len(mtx_dummy_testds),1))))\n",
    "\n",
    "#normalization\n",
    "mtx_dummy_tds_norm=normalizePhi(mtx_dummy_tds)\n",
    "mtx_dummy_testds_norm=normalizePhi(mtx_dummy_testds)\n",
    "\n",
    "\n",
    "#print(mtx_dummy_tds)\n",
    "#print(mtx_dummy_tds_norm)\n",
    "#pandas.get_dummies(trained_data.ix[:,1:2])\n",
    "#print(\"train\",np.shape(mtx_dummy_tds_norm),\"test\",np.shape(mtx_dummy_testds_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "td=dummy_trained_data;\n",
    "X = td;\n",
    "y = np.ravel(trained_y)\n",
    "\n",
    "index = [\"Logistic Regression\",\"Gaussian Navie Baise\",\"K Neighbors\"]\n",
    "\n",
    "columns = [\"Mean error\",\"Misclassified Points\", \"Accuracy\"]\n",
    "mc = pandas.DataFrame(index=index, columns=columns)\n",
    "\n",
    "lgr = 0, LogisticRegression(penalty='l2',max_iter=100)\n",
    "gnb = 1, GaussianNB()\n",
    "knn = 2, KNeighborsClassifier()\n",
    "\n",
    "classfier=lgr;\n",
    "trained_system=classfier[1].fit(X, y);\n",
    "y_pred = trained_system.predict(X)\n",
    "misclassifedPoints = (y_pred != y).sum()\n",
    "accuracy = (len(td.index) - misclassifedPoints) / len(td.index)\n",
    "mean=np.mean(np.abs(y_pred - y));\n",
    "mc.ix[classfier[0]] = [mean, misclassifedPoints, accuracy];\n",
    "test_pre=trained_system.predict(dummy_test_data);\n",
    "writeTestData(mtx_test_id,test_pre,filename=\"predictions_1\"); #0.60958\n",
    "\n",
    "classfier=gnb;\n",
    "trained_system=classfier[1].fit(X, y);\n",
    "y_pred = trained_system.predict(X)\n",
    "misclassifedPoints = (y_pred != y).sum()\n",
    "accuracy = (len(td.index) - misclassifedPoints) / len(td.index)\n",
    "mean=np.mean(np.abs(y_pred - y));\n",
    "mc.ix[classfier[0]] = [mean, misclassifedPoints, accuracy];\n",
    "test_pre=trained_system.predict(dummy_test_data);\n",
    "writeTestData(mtx_test_id,test_pre,filename=\"predictions_2\"); #0.63380\n",
    "\n",
    "\n",
    "classfier=knn;\n",
    "trained_system=classfier[1].fit(X, y);\n",
    "y_pred = trained_system.predict(X);\n",
    "misclassifedPoints = (y_pred != y).sum()\n",
    "accuracy = (len(td.index) - misclassifedPoints) / len(td.index)\n",
    "mean=np.mean(np.abs(y_pred - y));\n",
    "mc.ix[classfier[0]] = [mean, misclassifedPoints, accuracy];\n",
    "test_pre=trained_system.predict(dummy_test_data);\n",
    "writeTestData(mtx_test_id,test_pre,filename=\"predictions_3\"); #0.0.62932"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
