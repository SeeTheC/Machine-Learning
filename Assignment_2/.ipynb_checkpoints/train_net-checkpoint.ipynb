{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas;\n",
    "import numpy as np;\n",
    "import os.path;\n",
    "import os;\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readCSVFile(file):\n",
    "    data=pandas.read_csv(file,\",\",header=0, na_values='?', skipinitialspace=True);\n",
    "    return data;\n",
    "    pass;\n",
    "def readTrainData(dataset):    \n",
    "    return dataset.ix[:,1:-1], dataset.ix[:,-1:];\n",
    "    pass;\n",
    "\n",
    "def readTestData(dataset):    \n",
    "    return dataset.ix[:,1:],dataset.ix[:,0:1];\n",
    "    pass;\n",
    "\n",
    "def normalizePhi(unNormalizedPhi,last_col_bias=False):    \n",
    "    #assuming last column as bias column\n",
    "    no_of_column=len(unNormalizedPhi[0]);\n",
    "    phi=np.array(unNormalizedPhi);\n",
    "    std=phi.std(0);\n",
    "    mean=phi.mean(0);    \n",
    "    std[no_of_column-1]=1;\n",
    "    mean[no_of_column-1]=0;\n",
    "    #phi_normalize=(phi-mean)/std;    \n",
    "    \n",
    "    max_vector=phi.max(axis=0)\n",
    "    phi_normalize=phi/max_vector;    \n",
    "    \n",
    "    return phi_normalize;\n",
    "    pass;\n",
    "\n",
    "def writeTestData(test_id,ystar,filenumber=0,filename=None):\n",
    "    if(filename==None):\n",
    "        fo = open(\"log/output/sampleSubmission-\"+str(filenumber)+\".csv\", \"w\");               \n",
    "    else:\n",
    "        fo = open(filename+\".csv\", \"w\");        \n",
    "    fo.write(\"id,salary\\n\");\n",
    "    m=len(ystar);\n",
    "    for i in range(m):\n",
    "        fo.write(str(test_id[i][0])+\",\"+str(ystar[i][0])+\"\\n\");\n",
    "    fo.close();\n",
    "    pass;\n",
    "\n",
    "def dropColumns(dataframe,colList):\n",
    "    for c in colList:\n",
    "        dataframe.drop([c], axis = 1, inplace = True);\n",
    "    pass;\n",
    "\n",
    "def addColByCategory(dataset):\n",
    "    return pandas.get_dummies(dataset);\n",
    "    pass;\n",
    "\n",
    "def categoryToNumber(dataset,categoryList):\n",
    "    for c in categoryList:\n",
    "        if (c in dataset):            \n",
    "            dataset[c]=pandas.get_dummies(dataset[c]).values.argmax(1);        \n",
    "    return dataset;\n",
    "    pass;\n",
    "    \n",
    "\n",
    "def handleCategoryData(dataset,categoryList=None,byNumber=False):\n",
    "    if(byNumber):\n",
    "        return categoryToNumber(dataset,categoryList)\n",
    "    else:\n",
    "        return addColByCategory(dataset);\n",
    "    \n",
    "def findMostFrequentCount(dataset):\n",
    "    #arr=dataset;\n",
    "    #axis = 0\n",
    "    #u, indices = np.unique(arr, return_inverse=True)\n",
    "    #print(u);\n",
    "    #u[np.argmax(np.apply_along_axis(np.bincount, axis, indices.reshape(arr.shape),None, np.max(indices) + 1), axis=axis)]\n",
    "    x = np.array([0, 1, 1, 3, 2, 1, 2, 3]);\n",
    "    w=np.bincount(dataset[:,1].astype(int))#work-class: Private\n",
    "    o=np.bincount(dataset[:,6].astype(int))#occupation: Married-civ-spouse\n",
    "    c=np.bincount(dataset[:,13].astype(int))#country : US\n",
    "    pass;\n",
    "\n",
    "def fillNanValue(dataframe,col,value):\n",
    "    if (col in dataframe):\n",
    "        dataframe[col].fillna(value, inplace=True);\n",
    "    pass;\n",
    "\n",
    "def imputeUnknowValue(dataframe):\n",
    "    #by most frequent value;\n",
    "    fillNanValue(dataframe,\"workclass\",\"Private\");\n",
    "    fillNanValue(dataframe,\"occupation\",\"Craft-repair\");\n",
    "    fillNanValue(dataframe,\"native-country\",\"United-States\");\n",
    "    pass;\n",
    "\n",
    "def addRemainingCol(colList,dataframe,rowCount):\n",
    "    i=0;\n",
    "    for c in colList:\n",
    "        if( c not in dataframe):\n",
    "            dataframe.insert(i, c, 0);\n",
    "        i+=1;\n",
    "    pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NN:\n",
    "    logging_enabled=True;\n",
    "    trained_ds=None;\n",
    "    trained_output=None;\n",
    "    no_of_hiddenlayer=0;\n",
    "    neurons_per_hiddenlayer=0;    \n",
    "    weights=list();#list of matrix of weights\n",
    "    layer_output=list();#vector of layers\n",
    "    layer_delta=list(); \n",
    "    layer_error=list(); \n",
    "    no_of_iteration=1;\n",
    "    mean_print_rate=10000;\n",
    "    percent_print_rate=1000;\n",
    "    learning_rate=1;\n",
    "    approching_to_zero=1e-15;\n",
    "    approching_to_one=1-1e-15;\n",
    "    lamda=0.001;\n",
    "    log_dir=\"log\";\n",
    "    log_folder=None;\n",
    "    enable_bias_per_hidden=False;\n",
    "    def __init__(self):\n",
    "        np.random.seed(1);\n",
    "        if(self.logging_enabled):\n",
    "            self.log_folder=self.log_dir+\"/\"+self.getTimestamp();\n",
    "            self.createDir(self.log_folder);            \n",
    "        pass;\n",
    "    \n",
    "    def reInit(self):        \n",
    "        self.layer_output=[0]*(self.no_of_hiddenlayer+2);\n",
    "        self.layer_delta=[0]*(self.no_of_hiddenlayer+2);\n",
    "        self.layer_error=[0]*(self.no_of_hiddenlayer+2);        \n",
    "        pass;\n",
    "        \n",
    "    def createNN(self):\n",
    "        self.no_of_features=len(self.trained_ds[0]);\n",
    "        self.no_of_datapoint=len(self.trained_ds); \n",
    "        if(self.enable_bias_per_hidden and self.neurons_per_hiddenlayer>1):\n",
    "            self.neurons_per_hiddenlayer+=1;#adding bias neuron whose o/p will always be 1;\n",
    "            \n",
    "        self.reInit();\n",
    "        self.initWeightMatrix();       \n",
    "        self.log(\"Neural Network Created...\",\"-\");\n",
    "        pass;\n",
    "    \n",
    "    def initWeightMatrix(self):\n",
    "        self.weights=list();\n",
    "        for i in range(self.no_of_hiddenlayer):\n",
    "            if(i==0):\n",
    "                m=self.no_of_features;\n",
    "            else:\n",
    "                m=self.neurons_per_hiddenlayer;\n",
    "            w_matrix = 2*np.random.random((m,self.neurons_per_hiddenlayer)) - 1;            \n",
    "            \n",
    "            if(self.enable_bias_per_hidden):\n",
    "                #making weights of bias neuron as zero. So that its o/p is always one.\n",
    "                w_matrix[:,0]=0;\n",
    "                \n",
    "            self.weights.append(w_matrix);\n",
    "            #print(w_matrix);\n",
    "            \n",
    "        #last layer weight: For single output\n",
    "        if(self.no_of_hiddenlayer==0):\n",
    "            n=self.no_of_features;\n",
    "        else:\n",
    "            n=self.neurons_per_hiddenlayer;\n",
    "        w_vector=2*np.random.random((n,1)) - 1;\n",
    "        #print(w_vector);\n",
    "        self.weights.append(w_vector);\n",
    "        self.weights=np.array(self.weights);\n",
    "        pass;\n",
    "        \n",
    "    def getTimestamp(self):\n",
    "        import os.path;\n",
    "        import datetime;\n",
    "        import time;\n",
    "        ts = datetime.datetime.fromtimestamp(time.time()).strftime('%d-%m-%Y-%H:%M:%S')\n",
    "        return ts;\n",
    "\n",
    "    def createDir(self,directory):\n",
    "        import os.path;\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory);\n",
    "        pass;\n",
    "    \n",
    "    def writeWeights(self,filenumber=0):\n",
    "        fname=self.log_folder+\"/weights-\"+str(filenumber)+\"-\";\n",
    "        m=len(self.weights);\n",
    "        for i in range(m):\n",
    "            np.save(fname+\"\"+str(i), nn.weights[i])        \n",
    "        pass;\n",
    "    \n",
    "    def writeWeights(self,filenumber=0):\n",
    "        fname=self.log_folder+\"/weights-\"+str(filenumber)+\"-\";\n",
    "        m=len(self.weights);\n",
    "        for i in range(m):\n",
    "            np.save(fname+\"\"+str(i), self.weights[i])        \n",
    "        pass;\n",
    "    import os;\n",
    "\n",
    "\n",
    "    def writeFinalWeights(self):\n",
    "        filename=\"weights\";        \n",
    "        np.save(filename, self.weights);\n",
    "        os.rename(filename+\".npy\", filename+\".txt\");\n",
    "        pass;\n",
    "\n",
    "    \n",
    "    def loadWeights(self,folder=None,filename_offset=\"weights-0-\",filename=None):        \n",
    "        w=list();\n",
    "        i=0;\n",
    "        if(filename==None):\n",
    "            file_path=folder+\"/\"+filename_offset+str(i)+\".npy\";        \n",
    "            while (os.path.isfile(file_path)):\n",
    "                w.append(np.load(file_path));\n",
    "                i+=1;\n",
    "                file_path=folder+\"/\"+filename_offset+str(i)+\".npy\";\n",
    "            self.weights=w;\n",
    "        else:\n",
    "            if(os.path.isfile(filename)):\n",
    "                self.weights=np.load(filename);\n",
    "                w=self.weights;\n",
    "        return w;\n",
    "    pass\n",
    "\n",
    "    def activationFunction(self,x,deriv=False):\n",
    "        return self.actFunSigmoid(x,deriv);\n",
    "    \n",
    "    def actFunSigmoid(self,x,deriv=False):\n",
    "        if(deriv==True):\n",
    "            return x*(1-x)\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def actFunReLu(self,x,deriv=False):\n",
    "        p = x > 0;            \n",
    "        if(deriv==True):\n",
    "            return p.astype(int);\n",
    "        return x * p;\n",
    "    \n",
    "    def forwardPropogation(self,datapoints):        \n",
    "        #intial layer i.e l0 zeroth layer\n",
    "        self.layer_output[0]=datapoints;\n",
    "        for i in range(1,self.no_of_hiddenlayer+2):\n",
    "            prev_layer=self.layer_output[i-1];\n",
    "            w=self.weights[i-1]\n",
    "            sum_l=prev_layer.dot(w);           \n",
    "            l_i=self.activationFunction(sum_l);\n",
    "            self.layer_output[i]=l_i;   \n",
    "            if(self.enable_bias_per_hidden and i<=self.no_of_hiddenlayer):\n",
    "                #making weights of bias neuron as zero. So that its o/p is always one.\n",
    "                l_i[:,0]=1;\n",
    "            #print(\"l\"+str(i)+\":\",l_i);\n",
    "        return self.layer_output[self.no_of_hiddenlayer+1];\n",
    "        pass;\n",
    "    \n",
    "    def findError(self):\n",
    "        self.backPropogation(sel.layer_output);\n",
    "        return self.getMeanError();\n",
    "        \n",
    "    def backPropogation(self,l_output):\n",
    "        last_layer=len(l_output)-1;            \n",
    "        \n",
    "        #(target-output)        \n",
    "        li=l_output[last_layer];\n",
    "        error_diff=self.trained_output-li;\n",
    "        delta=error_diff*self.activationFunction(li,deriv=True)    \n",
    "        self.layer_delta[last_layer]=delta;\n",
    "        self.layer_error[last_layer]=error_diff;\n",
    "        \n",
    "        for i in range(last_layer-1,0,-1):            \n",
    "            #i-1 th layer calculation of delta\n",
    "            error_diff=self.layer_delta[i+1].dot(self.weights[i].T);\n",
    "            #print(\"l\"+str(i)+\"_error:\",delta);\n",
    "            delta=error_diff*self.activationFunction(l_output[i],deriv=True)\n",
    "            self.layer_delta[i]=delta;\n",
    "            self.layer_error[i]=error_diff;  \n",
    "            \n",
    "        #print(\"l\"+str(i)+\"_delta:\",delta);        \n",
    "        pass;\n",
    "    \n",
    "    def updateWeights(self,l_delta):\n",
    "        #print(self.learning_rate);\n",
    "        for i in range(self.no_of_hiddenlayer,-1,-1): # loop upto 0\n",
    "            #print(str(i)+\"==>a\",self.weights[i][0]);            \n",
    "            #print(str(i)+\"==>b\",self.layer_output[i].T.dot(l_delta[i+1])[0])\n",
    "            #print(str(i)+\"==>c\",(self.lamda*self.weights[i])[0])            \n",
    "            self.weights[i]+=self.learning_rate*self.layer_output[i].T.dot(l_delta[i+1]);\n",
    "            if(self.lamda!=0):\n",
    "                self.weights[i]-=self.learning_rate*-((self.lamda)*self.weights[i]);\n",
    "            #print(\"w\"+str(i)+\":\",self.layer_output[i].T.dot(l_delta[i+1]));\n",
    "    \n",
    "    \n",
    "    def getMeanError(self):   \n",
    "        return np.mean(np.abs(self.layer_error[self.no_of_hiddenlayer+1]));\n",
    "    \n",
    "    def getTrainedStatus(self,predicted_y):\n",
    "        index = [\"Neural-Network\"];\n",
    "        columns = [\"Mean\",\"Misclassified\", \"Accuracy\"];\n",
    "        status = pandas.DataFrame(index=index, columns=columns);       \n",
    "        t_y=self.getThresholdValue(predicted_y);\n",
    "        error_diff=self.trained_output-predicted_y;\n",
    "        mean=np.mean(np.abs(error_diff));\n",
    "        misclassifed= (self.trained_output != t_y).sum();\n",
    "        accuracy = (len(t_y) - misclassifed) / len(t_y);\n",
    "        status.ix[\"Neural-Network\"]=[mean,misclassifed,accuracy];\n",
    "        return status;\n",
    "    \n",
    "    def gradientDescent(self):\n",
    "        for i in range(self.no_of_iteration):\n",
    "            result=self.forwardPropogation(self.trained_ds);\n",
    "            self.backPropogation(self.layer_output);            \n",
    "            \n",
    "            if ((i%self.mean_print_rate==0 or i >=self.no_of_iteration-6)):\n",
    "                self.log(str(i)+\"Error:\",self.getMeanError());\n",
    "                self.log(self.getTrainedStatus(result));\n",
    "                \n",
    "            if (i%self.percent_print_rate==0 or i >=self.no_of_iteration-6):\n",
    "                self.log(str(i)+\"Percent completed\",(i*100)/self.no_of_iteration)\n",
    "                self.writeWeights();        \n",
    "            if(i!=self.no_of_iteration-1):#donot update for last iteration            \n",
    "                self.updateWeights(self.layer_delta);\n",
    "        \n",
    "        if self.logging_enabled:\n",
    "            print(\"Error:\",self.getMeanError());                      \n",
    "        pass;\n",
    "    \n",
    "    def getThresholdValue(self,result,threshold=0.5):\n",
    "        return(result>threshold).astype(int);\n",
    "\n",
    "    def train(self):\n",
    "         self.gradientDescent();\n",
    "            \n",
    "    def predict(self,datapoints):#forward propogation\n",
    "        loutput=[0]*(self.no_of_hiddenlayer+2);\n",
    "        #intial layer i.e l0 zeroth layer\n",
    "        loutput[0]=datapoints;\n",
    "        for i in range(1,self.no_of_hiddenlayer+2):\n",
    "            prev_layer=loutput[i-1];\n",
    "            w=self.weights[i-1]\n",
    "            sum_l=prev_layer.dot(w);           \n",
    "            l_i=self.activationFunction(sum_l);\n",
    "            loutput[i]=l_i;   \n",
    "            if(self.enable_bias_per_hidden and i<=self.no_of_hiddenlayer):\n",
    "                #making weights of bias neuron as zero. So that its o/p is always one.\n",
    "                l_i[:,0]=1;\n",
    "        return loutput[self.no_of_hiddenlayer+1];\n",
    "        pass;\n",
    "    \n",
    "    def log(self,text,data=None):\n",
    "        if self.logging_enabled:\n",
    "            if(data!=None):\n",
    "                print(text,data);\n",
    "            else:\n",
    "                print(text);\n",
    "        pass;\n",
    "pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--settings--\n",
    "pandas.set_option('display.max_columns', None);\n",
    "#---init---\n",
    "dir=\"\"\n",
    "trainFile=dir+\"train.csv\";\n",
    "testFile=dir+\"kaggle_test_data.csv\";\n",
    "categoryList=[\"workclass\",\"education\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"native-country\"];\n",
    "drop_col=[\"native-country\",\"race\",\"education\"]\n",
    "trained_dataset=readCSVFile(trainFile);\n",
    "trained_data,trained_y=readTrainData(trained_dataset);\n",
    "\n",
    "test_dataset=readCSVFile(testFile);\n",
    "test_data,test_id=readTestData(test_dataset);\n",
    "\n",
    "#droping unrelated-columns\n",
    "dropColumns(trained_data,drop_col);\n",
    "dropColumns(test_data,drop_col);\n",
    "\n",
    "#impute:\n",
    "imputeUnknowValue(trained_data);\n",
    "imputeUnknowValue(test_data);\n",
    "\n",
    "#converting categorical data to point wise data\n",
    "byNumber=False;\n",
    "dummy_trained_data=handleCategoryData(trained_data,categoryList,byNumber);\n",
    "dummy_test_data=handleCategoryData(test_data,categoryList,byNumber);\n",
    "\n",
    "\n",
    "#adding missing column\n",
    "trained_columns_name=list(dummy_trained_data.columns.values);\n",
    "addRemainingCol(trained_columns_name,dummy_test_data,len(trained_data))\n",
    "test_columns_name=list(dummy_test_data.columns.values);\n",
    "\n",
    "#converting panda data frame to numpy martix\n",
    "mtx_dummy_tds=dummy_trained_data.as_matrix(columns=None)\n",
    "mtx_dummy_testds=dummy_test_data.as_matrix(columns=None)\n",
    "mtx_trained_y=trained_y.as_matrix(columns=None);\n",
    "mtx_test_id=test_id.as_matrix(columns=None);\n",
    "\n",
    "#adding bias column\n",
    "mtx_dummy_tds=np.column_stack((mtx_dummy_tds,np.ones((len(mtx_dummy_tds),1))))\n",
    "mtx_dummy_testds=np.column_stack((mtx_dummy_testds,np.ones((len(mtx_dummy_testds),1))))\n",
    "\n",
    "#normalization\n",
    "mtx_dummy_tds_norm=normalizePhi(mtx_dummy_tds)\n",
    "mtx_dummy_testds_norm=normalizePhi(mtx_dummy_testds)\n",
    "\n",
    "\n",
    "#print(mtx_dummy_tds)\n",
    "#print(mtx_dummy_tds_norm)\n",
    "#pandas.get_dummies(trained_data.ix[:,1:2])\n",
    "#print(\"train\",np.shape(mtx_dummy_tds_norm),\"test\",np.shape(mtx_dummy_testds_norm));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Created... -\n",
      "0Error: 0.199313164915\n",
      "                    Mean Misclassified  Accuracy\n",
      "Neural-Network  0.199313          5530  0.858107\n",
      "0Percent completed 0.0\n",
      "15Error: 0.191606417564\n",
      "                    Mean Misclassified  Accuracy\n",
      "Neural-Network  0.191606          5495  0.859005\n",
      "15Percent completed 71.42857142857143\n",
      "16Error: 0.199305747928\n",
      "                    Mean Misclassified  Accuracy\n",
      "Neural-Network  0.199306          5528  0.858158\n",
      "16Percent completed 76.19047619047619\n",
      "17Error: 0.191605954801\n",
      "                    Mean Misclassified  Accuracy\n",
      "Neural-Network  0.191606          5495  0.859005\n",
      "17Percent completed 80.95238095238095\n",
      "18Error: 0.199304821207\n",
      "                    Mean Misclassified  Accuracy\n",
      "Neural-Network  0.199305          5528  0.858158\n",
      "18Percent completed 85.71428571428571\n",
      "19Error: 0.19160549211\n",
      "                    Mean Misclassified  Accuracy\n",
      "Neural-Network  0.191605          5495  0.859005\n",
      "19Percent completed 90.47619047619048\n",
      "20Error: 0.199303894574\n",
      "                    Mean Misclassified  Accuracy\n",
      "Neural-Network  0.199304          5528  0.858158\n",
      "20Percent completed 95.23809523809524\n",
      "Error: 0.199303894574\n",
      "predicted [[ 0.00190929]\n",
      " [ 0.17144102]\n",
      " [ 0.63958484]\n",
      " ..., \n",
      " [ 0.01877695]\n",
      " [ 0.02619357]\n",
      " [ 0.00100182]]\n",
      "Predicted-Th [[0]\n",
      " [0]\n",
      " [1]\n",
      " ..., \n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "Actual [[0]\n",
      " [0]\n",
      " [1]\n",
      " ..., \n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "#neural network\n",
    "#dataset: census income\n",
    "nn= NN();\n",
    "x=mtx_dummy_tds_norm;\n",
    "y=mtx_trained_y;\n",
    "nn.no_of_hiddenlayer=2;\n",
    "nn.neurons_per_hiddenlayer=20;\n",
    "nn.no_of_iteration=21;\n",
    "nn.learning_rate=0.0001;\n",
    "nn.lamda=0;\n",
    "nn.mean_print_rate=100;\n",
    "nn.percent_print_rate=100;\n",
    "nn.enable_bias_per_hidden=True;\n",
    "\n",
    "#Common setting\n",
    "nn.logging_enabled=True;\n",
    "nn.trained_ds=x;\n",
    "nn.trained_output=y;\n",
    "nn.createNN();\n",
    "nn.loadWeights(filename=\"weights.txt\");\n",
    "nn.train();\n",
    "nn.writeFinalWeights();\n",
    "result=nn.predict(x);\n",
    "print(\"predicted\",result);\n",
    "print(\"Predicted-Th\",nn.getThresholdValue(result));\n",
    "print(\"Actual\",y);\n",
    "result=nn.predict(mtx_dummy_testds_norm);\n",
    "writeTestData(mtx_test_id,result,filenumber=nn.getMeanError());\n",
    "writeTestData(mtx_test_id,nn.getThresholdValue(result),filename=\"predictions\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
