{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "from numpy.linalg import inv;\n",
    "from numpy.linalg import det;\n",
    "import math;\n",
    "\n",
    "# Will read the file and convert it into two dataset one train data other validate data\n",
    "def readTrainData(fileName):\n",
    "    row_index=0;\n",
    "    phi=list();\n",
    "    y=list();\n",
    "    with open(fileName) as f:\n",
    "        for line in f:\n",
    "            if row_index >0:\n",
    "                phi_i=list((float(n) for n in line.split('\\n')[0].split(\",\") ));\n",
    "                if(addW0Col):\n",
    "                    phi_i[0]=1;\n",
    "                else:# removing id col.\n",
    "                    phi_i.pop();\n",
    "                # last row is value of yi                \n",
    "                y_i=phi_i.pop(len(phi_i)-1); \n",
    "                phi.append(phi_i);             \n",
    "                y.append(y_i);\n",
    "            row_index+=1;\n",
    "    return [phi,y];\n",
    "#End-readTrainData\n",
    "\n",
    "# Will read the file and convert it into dataset for Testing the Model\n",
    "def readTestData(fileName):\n",
    "    row_index=0;\n",
    "    phi=list();\n",
    "    y=list();\n",
    "    with open(fileName) as f:\n",
    "        for line in f:\n",
    "            if row_index >0:                \n",
    "                phi_i=list((float(n) for n in line.split('\\n')[0].split(\",\") ));\n",
    "                if(addW0Col):\n",
    "                    phi_i[0]=1;\n",
    "                else:# removing id col.\n",
    "                    phi_i.pop();                \n",
    "                phi.append(phi_i);                             \n",
    "            row_index+=1;\n",
    "    m=len(phi);    \n",
    "    return phi;\n",
    "#End-readTrainData\n",
    "\n",
    "\n",
    "\n",
    "#write-output\n",
    "def writeTestData(ystar):\n",
    "    fo = open(\"output.csv\", \"w\");    \n",
    "    fo.write(\"ID,MEDV\\n\");\n",
    "    m=len(ystar);\n",
    "    for i in range(m):\n",
    "        fo.write(str(i)+\",\"+str(ystar[i])+\"\\n\");\n",
    "    fo.close();\n",
    "    pass;\n",
    "\n",
    "# Return det of matrix\n",
    "def getDet(A):\n",
    "    d=det(A);\n",
    "    if(d<10**-10):\n",
    "        return 0;\n",
    "    return d;\n",
    "\n",
    "\n",
    "#Return RMS: root mean square error\n",
    "def getRMS(y,yStar):\n",
    "    m=len(y);\n",
    "    sigma=0;\n",
    "    for i in range(m):\n",
    "        delta=(y[i]-yStar[i]);\n",
    "        delta=delta*delta;\n",
    "        sigma=sigma+delta;\n",
    "    meanSq=sigma/m;   \n",
    "    rms=math.sqrt(meanSq);\n",
    "    return rms;\n",
    "    pass;\n",
    "\n",
    "#For ploting graph of RMS VS Iteration\n",
    "def plotGraph(x,y):\n",
    "    import matplotlib.pyplot as plt;\n",
    "    plt.plot(x,y)\n",
    "    plt.ylabel('rms')\n",
    "    plt.xlabel('iteration');\n",
    "    plt.show();\n",
    "    pass;\n",
    "\n",
    "#Record readings for gradient descent\n",
    "def writeReadingInFile(filename,alpha,lam,iteration,rms,p):\n",
    "    import os.path;\n",
    "    import datetime;\n",
    "    import time;\n",
    "    ts = datetime.datetime.fromtimestamp(time.time()).strftime('%d-%m-%Y %H:%M:%S')\n",
    "    if(os.path.exists(filename)==False):\n",
    "        fo = open(filename, \"w\"); \n",
    "        fo.write(\"iteration,norm,alpha,lam,rms,timestamp\\n\");\n",
    "        fo.write(str(iteration)+\",\"+str(p)+\",\"+str(alpha)+\",\"+str(lam)+\",\"+str(rms)+\",\"+str(ts)+\"\\n\");\n",
    "    else:\n",
    "        fo = open(filename, \"a\"); \n",
    "        fo.write(str(iteration)+\",\"+str(p)+\",\"+str(alpha)+\",\"+str(lam)+\",\"+str(rms)+\",\"+str(ts)+\"\\n\");\n",
    "    fo.close();                    \n",
    "    pass;\n",
    "\n",
    "\n",
    "#normalize the data set ny (x-u)/s where s is max-min\n",
    "def normalizePhi(unNormalizedPhi):    \n",
    "    phi=np.array(unNormalizedPhi);\n",
    "    print(\"Normalizing Phi...\");  \n",
    "    std=phi.std(0);\n",
    "    mean=phi.mean(0); \n",
    "    if(addW0Col):#making first col. mean as 0              \n",
    "        std[0]=1;\n",
    "        mean[0]=0;\n",
    "    phi_normalize=(phi-mean)/std;    \n",
    "    print(\"Normalization done.\");\n",
    "    return phi_normalize;\n",
    "    pass;\n",
    "\n",
    "#pridict of y* given w* QW=y*\n",
    "def pridict(dataset,weight):\n",
    "    phi=np.array(dataset);\n",
    "    w=np.array(weight);\n",
    "    ystar=np.dot(phi,w);\n",
    "    return ystar;\n",
    "    pass;\n",
    "\n",
    "# Finding w*=(QTQ)^-1QTY\n",
    "def trainUsingClosedFormEquation(dataset,output):\n",
    "    m=len(dataset);\n",
    "    n=len(dataset[0]);\n",
    "    phi=np.array(dataset);\n",
    "    y=np.array(output);\n",
    "    phiT=np.transpose(phi);\n",
    "    #(QTQ)    \n",
    "    phiT_phi=np.dot(phiT,phi); \n",
    "    d=getDet(phiT_phi)\n",
    "    if(True or d>0):\n",
    "        #(QTQ)^-1\n",
    "        phiT_phi_inv=inv(phiT_phi);\n",
    "        #(QTQ)^-1QT\n",
    "        phiT_phi_inv_phiT=np.dot(phiT_phi_inv,phiT);  \n",
    "        #(QTQ)^-1QT*Y\n",
    "        w=np.dot(phiT_phi_inv_phiT,y);      \n",
    "        return w;   \n",
    "    else:\n",
    "        print(\"Error:Phi is NOT full column rank.\");\n",
    "        return None;\n",
    "    pass;\n",
    "\n",
    "def numpiTestFun():\n",
    "    A2= np.matrix([[4,6],[2,8]])        \n",
    "    A3= np.matrix([[1,2,3],[4,5,7],[7,8,9]])\n",
    "    A=A2;\n",
    "    print(A);\n",
    "    print(np.power(A,0.5));\n",
    "    print(A);\n",
    "    print(\"Det(A):\"+str(getDet(A)));\n",
    "    B= np.transpose(A);\n",
    "    C=inv(A);\n",
    "    #print(C);\n",
    "    print(np.dot(A,C));\n",
    "    print(A.std(0));\n",
    "    print(A.mean(0));\n",
    "    print(normalizePhi(A));\n",
    "    norm=(A-A.mean(0))/A.std(0);    \n",
    "    print(norm);    \n",
    "    print();\n",
    "    pass;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 12 156]\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "# GD: Least Sq. Without Regularlization\n",
    "def gardientDescentErrorFun(phi,y):\n",
    "    m=len(y);#no of data points\n",
    "    n=len(phi[0]);# no. of features    \n",
    "    alpha=0.22;# learning parameter\n",
    "    maxIteration=10000;\n",
    "    phi=np.array(phi);\n",
    "    y=(np.array(y));#converting row vector to col vector    \n",
    "    wk0=np.zeros(n);# Nx1 vector\n",
    "    phiT=np.transpose(phi);\n",
    "    phiTphi=np.dot(phiT,phi);   \n",
    "    phiTy=np.dot(phiT,y);   \n",
    "    alphaBym=alpha/m;\n",
    "    xaxis=list();\n",
    "    yaxis=list();\n",
    "    #----------------------\n",
    "    print(\"Training Started (Least Sq. Without Regularlization) ...\");\n",
    "    for i in range(maxIteration):  \n",
    "        wk1=wk0-(alphaBym*((np.dot(phiTphi,wk0)-phiTy)));                \n",
    "        ystar=pridict(phi,wk1);\n",
    "        rms=getRMS(y,ystar);    \n",
    "        xaxis.append(i);\n",
    "        yaxis.append(rms);\n",
    "        percentComplete=((i+1)*100)/maxIteration;\n",
    "        if( percentComplete%10==0 ):\n",
    "            print(\"Percent Completed\",percentComplete);\n",
    "        wk0=wk1;\n",
    "    print(\"Final Trained RMS:\",rms);\n",
    "    plotGraph(xaxis,yaxis);\n",
    "    return wk1;\n",
    "    pass;\n",
    "\n",
    "\n",
    "\n",
    "#wStart=gardientDescentWithRidge(trainDatasetPhi,trainDatasetY);\n",
    "#wStart=gardientDescentWithPnom(trainDatasetPhi,trainDatasetY,4);\n",
    "#mainRidgeClosedFormSol();\n",
    "\n",
    "a=np.array([[3,4],[12,13]]);\n",
    "b=np.array([3,4]);\n",
    "c=a-b;\n",
    "c=np.sum(a,axis=1);\n",
    "sigma=1;\n",
    "c=np.power(c,2);\n",
    "c=c*(-1*(1/sigma));\n",
    "c=np.exp(c);\n",
    "d=np.insert(a,len(a[0]),b,axis=1);\n",
    "print(a[:,1]*a[:,0]);\n",
    "print(np.linalg.norm(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Trained Dataset from file...\n",
      "Normalizing Phi...\n",
      "Normalization done.\n",
      "Normalizing Phi...\n",
      "Normalization done.\n",
      "Fetching of data Completed.\n",
      "Train Size:280\n",
      "Validate Size:120\n",
      "Training Started (Least Sq. With Ridge) ...\n",
      "Percent Completed 10.0  rms: 2.4076753125541246\n",
      "Percent Completed 20.0  rms: 2.400390460655885\n",
      "Percent Completed 30.0  rms: 2.398125407071973\n",
      "Percent Completed 40.0  rms: 2.3971979520285513\n",
      "Percent Completed 50.0  rms: 2.396666280152828\n",
      "Percent Completed 60.0  rms: 2.3962770293783278\n",
      "Percent Completed 70.0  rms: 2.395954688344699\n",
      "Percent Completed 80.0  rms: 2.395671952525709\n",
      "Percent Completed 90.0  rms: 2.3954160184827993\n",
      "Percent Completed 100.0  rms: 2.3951793940893125\n",
      "Final Trained RMS: 2.3951793940893125 . Iteration needed  99999\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAF5CAYAAAB9WzucAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3XmUXWWZ7/HvE5IIEgkKGgaRQQUTRTCFjYARaXDodAPO\nWGILQbyiTauhW5S+IuKAtl4ZBHGipVWgHBd022iDKJGLQCMpQS+EIJOoQACBigYiIXnuH+8uc3Ko\nVKUqp7JP7Xw/a+11zn73u/d5zpuhfrXHyEwkSZKaZlLdBUiSJI0HQ44kSWokQ44kSWokQ44kSWok\nQ44kSWokQ44kSWokQ44kSWokQ44kSWokQ44kSWokQ44kSWqk2kNORJwQEddGxNKIWBIRF0bEruuw\n3tSI+ERE3BkRyyPi9og4smX5ERGxKiJWVq+rIuKRcf0ykiSpa0yuuwBgDnAmcB2lnk8Cl0bEzMx8\ndJj1vgM8HZgH3AZsyxND2wCwKxDVvA/qkiRpI1F7yMnMua3z1d6Y+4Ae4Mqh1omIV1PC0S6Z+XDV\nfNfQm8/7O1etJEmaKGo/XDWELSl7XB4cps/BlD0/H4iI30XE4oj4TERs2tZvWnU4666IuCgiZo1X\n0ZIkqbvUvienVUQEcDpwZWbeNEzXXSh7cpYDrwG2Br4APA14e9VnMXAU8EtgOvB+4KqImJWZd4/P\nN5AkSd0iMrvnNJWI+ALwKmC/zLxnmH6XAC8FZmTmn6q211LO09k8M/88xDqTgUXABZl50lq2u1X1\n+XdSApQkSVo3mwI7AZdk5h9qrgXooj05EXEWMBeYM1zAqdwD/H4w4FQWUU4wfiblROQ1ZObjEfEL\n4DnDbPdVwPmjKlySJLU6HLig7iKgS0JOFXAOBfbPzKFOIG73M+ANEfHkzBy8LHw3YBXwu7V8xiRg\nd+DiYbZ7J8B5553HzJkz17F6ra/58+dz2mmn1V3GRsUx3/Ac8w3PMd+wFi1axFvf+laofpZ2g9pD\nTkScDfQChwDLImJGtWggM5dXfU4Bts/MI6plFwAfAs6NiI9QLiX/NPBvg4eqIuJE4BrgVsrJzMcD\nzwLOGaac5QAzZ85k9uzZHfuOGt706dMd7w3MMd/wHPMNzzGvTdec7tENV1cdA2wBLADubpne1NJn\nW2CHwZnMXAa8ghJefg58A/gP4L0t6zwV+DJwE2XvzTRgn8y8eZy+hyRJ6iK178nJzBGDVmbOG6Lt\nFso5NGtb5zjguPWrTpIkTVTdsCdHkiSp4ww5ql1vb2/dJWx0HPMNzzHf8BxzddV9cuoWEbOBhQsX\nLvRkNUmSRqG/v5+enh6Anszsr7secE+OJElqKEOOJElqJEOOJElqJEOOJElqJEOOJElqJEOOJElq\nJEOOJElqJEOOJElqJEOOJElqJEOOJElqJEOOJElqJEOOJElqJEOOJElqJEOOJElqJEOOJElqJEOO\nJElqJEOOJElqJEOOJElqJEOOJElqJEOOJElqJEOOJElqJEOOJElqJEOOJElqJEOOJElqJEOOJElq\nJEOOJElqJEOOJElqJEOOJElqJEOOJElqJEOOJElqJEOOJElqJEOOJElqJEOOJElqJEOOJElqJEOO\nJElqJEOOJElqJEOOJElqJEOOJElqpNpDTkScEBHXRsTSiFgSERdGxK7rsN7UiPhERNwZEcsj4vaI\nOLKtzxsjYlFEPBoRN0TE34zbF5EkSV2l9pADzAHOBPYGDgKmAJdGxGYjrPcd4ABgHrAr0AssHlwY\nEfsCFwBfAfYE/gO4KCJmdfoLSJKk7jO57gIyc27rfLU35j6gB7hyqHUi4tWUcLRLZj5cNd/V1u09\nwA8z89Rq/sMR8QrgWODdnalekiR1q27Yk9NuSyCBB4fpczBwHfCBiPhdRCyOiM9ExKYtffYBLmtb\n75KqXZIkNVzte3JaRUQApwNXZuZNw3TdhbInZznwGmBr4AvA04C3V322AZa0rbekapckSQ3XVSEH\nOBuYBew3Qr9JwCrgLZn5J4CIOA74TkS8OzP/vD5FzJ8/n+nTp6/R1tvbS29v7/psVpKkRujr66Ov\nr2+NtoGBgZqqWbuuCTkRcRYwF5iTmfeM0P0e4PeDAaeyCAjgmcBtwL3AjLb1ZlTtwzrttNOYPXv2\nupYuSdJGZahf/Pv7++np6ampoqF1xTk5VcA5FDggM9tPIB7Kz4DtIuLJLW27Ufbu/K6avxo4sG29\nV1TtkiSp4WoPORFxNnA48BZgWUTMqKZNW/qcEhFfa1ntAuAPwLkRMTMiXgZ8Gvi3lkNVZwCvjojj\nImK3iPgI5YqtszbA15IkSTWrPeQAxwBbAAuAu1umN7X02RbYYXAmM5dR9spsCfwc+AblPjjvbelz\nNSU4/S/geuB1wKEjnNAsSZIaovZzcjJzxKCVmfOGaLsFeNUI630P+N7Yq5MkSRNVN+zJkSRJ6jhD\njiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJ\naiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRD\njiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJ\naiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRD\njiRJaqTaQ05EnBAR10bE0ohYEhEXRsSuI6yzf0SsaptWRsQzWvoc0dI+2OeR8f9GkiSpG0yuuwBg\nDnAmcB2lnk8Cl0bEzMx8dJj1EtgV+ONfGjLva+szUPWJlnUkSdJGoPaQk5lzW+cj4kjgPqAHuHKE\n1e/PzKXDbz7vX78KJUnSRFT74aohbEnZ4/LgCP0CuD4i7o6ISyNi3yH6TIuIOyPiroi4KCJmdbxa\nSZLUlboq5EREAKcDV2bmTcN0vQd4J/B64HXAb4EFEbFnS5/FwFHAIcDhlO96VURsNx61S5Kk7lL7\n4ao2ZwOzgP2G65SZtwC3tDRdExHPBuYDR1R9rgGuGewQEVcDiyjh6KTOli1JkrpN14SciDgLmAvM\nycx7xrCJaxkmHGXm4xHxC+A5I21o/vz5TJ8+fY223t5eent7x1CWJEnN0tfXR19f3xptAwMDNVWz\ndpFZ/wVHVcA5FNg/M28f4zYuBZZm5hvWsnwScCNwcWb+81r6zAYWLly4kNmzZ4+lDEmSNkr9/f30\n9PQA9GRmf931QBfsyYmIs4FeyrkzyyJiRrVoIDOXV31OAbbPzCOq+fcCd1BCy6bAO4ADgFe0bPdE\nyuGqWyknMx8PPAs4ZwN8LUmSVLPaQw5wDOVqqgVt7fOAr1fvtwV2aFk2FfgssB3wCPBL4MDMvKKl\nz1OBLwPbAA8BC4F9MvPmDtcvSZK6UO0hJzNHvMIrM+e1zX8G+MwI6xwHHLd+1UmSpImqqy4hlyRJ\n6hRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRD\njiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJ\naiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRD\njiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaiRDjiRJaqQxhZyImB0Ru7fM\nHxoRF0XEKRExtXPlSZIkjc1Y9+R8CdgVICJ2Ab4JPAK8Efh0Z0qTJEkau7GGnF2B66v3bwSuyMy3\nAEcCr+9AXZIkSetlrCEnWtY9CPhB9f63wNbrW5QkSdL6GmvIuQ74UET8PbA/cHHVvjOwZDQbiogT\nIuLaiFgaEUsi4sKI2HWEdfaPiFVt08qIeEZbvzdGxKKIeDQiboiIvxlNbZIkaeIaa8h5HzAbOAv4\nRGbeWrW/AbhqlNuaA5wJ7E3ZKzQFuDQiNhthvQSeC2xTTdtm5n2DCyNiX+AC4CvAnsB/ABdFxKxR\n1idJkiagyWNZKTN/Cew+xKL3AytHua25rfMRcSRwH9ADXDnC6vdn5tK1LHsP8MPMPLWa/3BEvAI4\nFnj3aGqUJEkTz3rfJycipkXEFhGxBTAVGGkPzEi2pOyleXCkjwauj4i7I+LSas9Nq32Ay9raLqna\nJUlSw431Pjk7R8TFEbEMGAAeqqaHq9cxiYgATgeuzMybhul6D/BOypVcr6Oc8LwgIvZs6bMNTzw/\naEnVLkmSGm5Mh6uA8yh7Uo6iBIfsUD1nA7OA/YbrlJm3ALe0NF0TEc8G5gNHdKgWSZI0gY015OwB\n9GTm4k4VEhFnAXOBOZl5zxg2cS1rhqN7gRltfWZU7cOaP38+06dPX6Ott7eX3t7eMZQlSVKz9PX1\n0dfXt0bbwMBATdWsXWSOfidMRFxOuaqq/ZyXsRVRAs6hwP6ZefsYt3EpsDQz31DNfxPYLDMPbenz\nM+CGzBzyxOOImA0sXLhwIbNnzx5LGZIkbZT6+/vp6emBshOkv+56YOx7co4GvhgR2wP/D1jRurC6\n+mqdRMTZQC9wCLAsIgb3vgxk5vKqzynA9pl5RDX/XuAO4EZgU+AdwAHAK1o2fQblPJ3jKPfx6aVc\nsfWO0X1VSZI0EY015DwdeDZwbktbUs7TSWCTUWzrmGqdBW3t84CvV++3BXZoWTYV+CywHeWZWb8E\nDszMK/5STObVEfEW4BPV9Gvg0BFOaJYkSQ0x1pDzVeAXlL0j63XicWaOeIVXZs5rm/8M8Jl1WO97\nwPfGWpskSZq4xhpydgQOabnTsSRJUlcZ680Af0K5wkqSJKkrjXVPzveB0yJid+BXPPHE4/9c38Ik\nSZLWx1hDzher1w8PsWy0Jx5LkiR13KgPV0XEFMqVUM/LzElDTAYcSZJUu1GHnMxcQXkC+arOlyNJ\nktQZYz3x+DzKDQElSZK60ljPyZkMHBURBwELgWWtCzPzuPUtTJIkaX2MNeS8ABh8LsWubcs69URy\nSZKkMRtTyMnMAzpdiCRJUieN9ZwcSZKkrmbIkSRJjWTIkSRJjWTIkSRJjWTIkSRJjWTIkSRJjWTI\nkSRJjWTIkSRJjWTIkSRJjWTIkSRJjWTIkSRJjWTIkSRJjWTIkSRJjWTIkSRJjWTIkSRJjWTIkSRJ\njWTIkSRJjWTIkSRJjWTIkSRJjWTIkSRJjWTIkSRJjWTIkSRJjWTIkSRJjWTIkSRJjWTIkSRJjWTI\nkSRJjWTIkSRJjWTIkSRJjWTIkSRJjWTIkSRJjWTIkSRJjVR7yImIEyLi2ohYGhFLIuLCiNh1FOvv\nFxErIqK/rf2IiFgVESur11UR8Ujnv4EkSepGtYccYA5wJrA3cBAwBbg0IjYbacWImA58DbhsLV0G\ngG1aph07UbAkSep+k+suIDPnts5HxJHAfUAPcOUIq38ROB9YBRw69Obz/g6UKUmSJphu2JPTbksg\ngQeH6xQR84CdgZOH6TYtIu6MiLsi4qKImNXBOiVJUhfrqpATEQGcDlyZmTcN0++5wCnA4Zm5ai3d\nFgNHAYcAh1O+61URsV1nq5YkSd2o9sNVbc4GZgH7ra1DREyiHKI6KTNvG2xu75eZ1wDXtKx3NbAI\neCdw0nBFzJ8/n+nTp6/R1tvbS29v77p9C0mSGqyvr4++vr412gYGBmqqZu0iM+uuAYCIOAs4GJiT\nmXcN02868BDwOKvDzaTq/ePAKzNzwVrW/TawIjMPX8vy2cDChQsXMnv27LF+FUmSNjr9/f309PQA\n9GRm/0j9N4Su2JNTBZxDgf2HCziVpcAL2tr+ATgAeD1w51o+YxKwO3DxehUrSZImhNpDTkScDfRS\nzp1ZFhEzqkUDmbm86nMKsH1mHpFl19NNbdu4D1iemYta2k6kHK66lXIy8/HAs4BzxvkrSZKkLlB7\nyAGOoVxNtaCtfR7w9er9tsAOo9zuU4EvU+6P8xCwENgnM28ec6WSJGnCqD3kZOaIV3hl5rwRlp9M\n26XkmXkccNz6VSdJkiaqrrqEXJIkqVMMOZIkqZEMOZIkqZEMOZIkqZEMOZIkqZEMOZIkqZEMOZIk\nqZEMOZIkqZEMOUNYvrzuCiRJ0voy5Azh7rvrrkCSJK0vQ84Qrrqq7gokSdL6MuQM4ZxzYPHiuquQ\nJEnrw5AzhK23hle+Eu64o+5KJEnSWBlyhnD22TB1KrzsZfDrX9ddjSRJGgtDzhCe8Qz46U9h2jTY\nf39YtKjuiiRJ0mgZctZiu+1gwQLYaqsSdPr7665IkiSNhiFnGDNmwOWXw047lUNX//3fdVckSZLW\nlSFnBFtvXYLOAQfA3/0dfPWrdVckSZLWhSFnHWy+OVx4IRx9NLz97fChD8GqVXVXJUmShjO57gIm\nismT4QtfgF12gQ9+EH7xCzj/fNhyy7orkyRJQ3FPzihEwPHHww9+UO6K/OIXw4031l2VJEkaiiFn\nDF79arjuOthsM9h7b/j3f4fMuquSJEmtDDlj9Oxnw9VXw5veBPPmwZvfDA8/XHdVkiRpkCFnPWy+\nebna6pvfhEsugT32KDcRlCRJ9TPkdMBhh8ENN8CzngUvfzm8612wdGndVUmStHEz5HTIjjuWOyR/\n7nPwjW/A858P//VfdVclSdLGy5DTQZtsAv/4j+WKqxe8AA4+GF73Orj99rorkyRp42PIGQc77lgu\nM7/gAvj5z2HmTPiXf4E//rHuyiRJ2ngYcsZJBPT2ws03l5sHnnYa7LYbfPGL8NhjdVcnSVLzGXLG\n2eabw8knl7Dz138N7353CTvnnguPP153dZIkNZchZwPZcUc47zz41a9gr73gqKNg1qxyCfqf/1x3\ndZIkNY8hZwN7/vPhO9+B/v4Sct7+dthpJ/jkJ+Ghh+quTpKk5jDk1ORFL4KLLoJFi8pVWCefDDvs\nAMceW/b2SJKk9WPIqdnzngdf/jL85jcwfz5897vwwhfCvvvC174GjzxSd4WSJE1MhpwuMWMGfOxj\n8NvflqAzbRoceSRstx0cfTT85CewcmXdVUqSNHEYcrrMlCnw+tfDpZfCbbeVw1eXXw4HHlgOZx13\nHFx7LaxaVXelkiR1N0NOF9tlF/j4x+HWW+Gaa+CNb4Tzz4e99y6B55hjyk0Hly+vu1JJkrqPIWcC\niCjB5owz4Pe/L4euDjsMfvQj+Nu/ha22gte8Bj7/eVi8GDLrrliSpPoZciaYyZPhgAPg1FPLHp4b\nb4QTT4QHHoD3va+cyLzDDnDEEeVBoXfdZeiRJG2cJtddgMYuotxrZ9as8uiIP/0JrrgCfvzjMn39\n66XfdtvBPvusnmbPhk03rbd2SZLGW+0hJyJOAF4LPA94FLgK+EBm3rKO6+8HLAB+lZmz25a9Efgo\nsBNwC/DBzPxhx4rvMtOmwdy5ZYKyd+eqq8p09dVlj8+jj8LUqeUy9T33XD298IXwlKfUW78kSZ1U\ne8gB5gBnAtdR6vkkcGlEzMzMR4dbMSKmA18DLgNmtC3bF7gA+ABwMXA4cFFEvCgzb+r4t+hCW28N\nhxxSJoAVK+CGG0rg6e+H664r9+JZsaIsf85zYI89yp6h5z2vTLvtVp6/JUnSRFN7yMnMua3zEXEk\ncB/QA1w5wupfBM4HVgGHti17D/DDzDy1mv9wRLwCOBZ493qWPSFNmVKem7XXXqvbHnsMbroJrr++\nTDfcAF/5Ctx77+o+O+ywOvTssgvsvHOZdtoJtthig38NSZLWSe0hZwhbAgk8OFyniJgH7EzZQ3Pi\nEF32AT7b1nYJTwxDG7WpU1cfsmo1MFCu1Lr55tXTZZfBHXesecn6055Wws5g6HnmM8s5QNtvX163\n3dbzfyRJ9eiqkBMRAZwOXDncIaWIeC5wCvDSzFxVVnuCbYAlbW1LqnaNYPp0+Ku/KlOrTFiypISd\nO+8sr4PvL7qoXOLeft+epz2tBJ7BacaMciht663h6U9f8/UpTyknVEuStL66KuQAZwOzgP3W1iEi\nJlEOUZ2UmbcNNm+A2kQJINtsU6Z99nni8kx4+GG4++6hp5tvLleAPfAALF36xPWnTl0dgLbaCrbc\nsgSu1te1vd9ii3KJvSRJ0EUhJyLOAuYCczLznmG6PgXYC9gzIj5ftU0qm4jHgFdm5gLgXtpORq7m\n72UE8+fPZ/r06Wu09fb20tvbuy5fZaMWAU99apme//zh+z72WAk7DzwA99//xNcHHyyHze69twSn\ngYHyOtxDS5/0pHKi9LRpo3vdbLMybbrp6ql1vvX91KnubZK0cevr66Ovr2+NtoGBgZqqWbvILrhT\nXBVwDgX2z8zbR+gbwMy25n8ADgBeD9yZmY9GxDeBzTLz0JZ1fwbckJlDnngcEbOBhQsXLmT27NlD\ndVEXWLFideBpfR0YgGXLyv2C1vY6VNtoH4sRsWYYag1BU6eunqZMWfv8cMvW1nfKlDJNngybbFJe\nh5pGWjbJW4BKGgf9/f309PQA9GRmf931QBfsyYmIs4Fe4BBgWUQM7n0ZyMzlVZ9TgO0z84gsqeym\ntm3cByzPzEUtzWcACyLiOMol5L2UK7beMa5fSONuypTVh7Q6YeXKEnSWLy/3ERp8P9r5Rx8te6dW\nrCivg+8feeSJ7UP1a102niLGFpAGlw8GpXV9v77Lx3tbEeX9ukyd7BvhHkFpvNUecoBjKFdTLWhr\nnwdU9+xlW2CH0Ww0M6+OiLcAn6imXwOHbiz3yNG622STcsiqW+4HlFmC11DhZ+VKePzxMrW+H2oa\nbvn6rrtq1Zqvg+8Ha2xvH8v7de07kQ0GnTpC1lj6rmtwa+071Ou69On09ur4zG7f3sYQsmsPOZk5\naR36zBth+cnAyUO0fw/43tirkza81j0tGtloQ1H766pVJVgOvh9pqrvvhv78xx9ft74rV5b2wam1\n31Cv69JnNH1HWqahdTI0Pf543d/mifxvVNKENrgXQRpOJ0NTHUFtImzv7rvhnHPq/pNekyFHktR4\nEeXQtMZPf3/3hRx//5EkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEk\nSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1k\nyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEk\nSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1kyJEkSY1k\nyJEkSY1kyJEkSY1kyJEkSY1Ue8iJiBMi4tqIWBoRSyLiwojYdYR19ouIKyPigYh4JCIWRcT72voc\nERGrImJl9boqIh4Z32+jsejr66u7hI2OY77hOeYbnmOu2kMOMAc4E9gbOAiYAlwaEZsNs86yap05\nwPOAjwEfj4ij2/oNANu0TDt2tnR1gv8RbXiO+YbnmG94jrkm111AZs5tnY+II4H7gB7gyrWscz1w\nfUvTBRHxekroOWfNrnl/RwuWJEkTQjfsyWm3JZDAg+u6QkS8CNgHWNC2aFpE3BkRd0XERRExq3Nl\nSpKkbtZVISciAjgduDIzb1qH/r+NiOXAtcDnM/PclsWLgaOAQ4DDKd/1qojYrvOVS5KkblP74ao2\nZwOzgP3Wsf9LgWnAS4B/jYhbM/NbAJl5DXDNYMeIuBpYBLwTOGkt29sUYNGiRWMqXmMzMDBAf39/\n3WVsVBzzDc8x3/Ac8w2r5WfnpnXW0Soys+4aAIiIs4CDgTmZedcY1v/fwFszc+Ywfb4NrMjMw9ey\n/C3A+aP9bEmS9BeHZ+YFdRcBXbInpwo4hwL7jyXgVDYBnjTMZ0wCdgcuHmYbl1AObd0JLB9jHZIk\nbYw2BXai/CztCrWHnIg4G+ilnDuzLCJmVIsGMnN51ecUYPvMPKKafzdwF3Bz1Xd/4J8o5/MMbvdE\nyuGqWyknMx8PPIs1r75aQ2b+AeiK9ClJ0gR0Vd0FtKo95ADHUK6mWtDWPg/4evV+W2CHlmWTgE9S\nEuPjwG3A+zPzyy19ngp8mXJ/nIeAhcA+mXkzkiSp8brmnBxJkqRO6qpLyCVJkjrFkCNJkhrJkFOJ\niH+IiDsi4tGIuCYiXlx3Td1mXR+mGhEfjYi7q4en/igintO2/EkR8fnqAat/jIjvRsQz2vo8NSLO\nj4iBiHgoIs6JiM3b+uwQERdHxLKIuDciPl1dRddYEfHB6mGzp7a1O+YdFBHbRcQ3Wh4CfENEzG7r\n45h3SERMioiPRcTt1XjeGhEfGqKfYz5GETEnIv4zIn5f/R9yyBB9umZ8I+KFEXFFlJ/Jv4mI94/p\ni2fmRj8Bh1EuGX8b5YGfX6I8VmLrumvrpgn4AfD3wEzK5fj/RbncfrOWPh+oxu7vgBcAF1FODJ/a\n0ucL1Xr7Ay+inI3/f9s+64dAP7AXsC9wC3Bey/JJwK8olyruDryK8syzj9c9TuM4/i8Gbgd+AZzq\nmI/bOG8J3EG5ErOH8mDfg4CdHfNxG/N/qb7XqylXwb4OWAoc65h3bIxfDXyUcruWlcAhbcu7ZnyB\npwD3AF+j/Lx5E+XB3EeP+nvXPfDdMFEuNT+jZT6A3wHH111bN0/A1sAq4KUtbXcD81vmtwAeBd7U\nMv9n4LUtfXartvNX1fzMav5FLX1eRbmSbptq/m+AFbQEUcrdrB8CJtc9NuMw1tMojyr5a+By1gw5\njnlnx/pTwE9H6OOYd3bMvw98pa3tu8DXHfNxGe9VPDHkdM34Au8CHmgdb8oV1TeN9rtO6N1vnRAR\nUyi/rf14sC3LiF5Geein1m6Nh6lGxM6US/Zbx3Ip8D+sHsu9KLcuaO2zmHLfo8E+LwEeysxftHzW\nZdVn7d3S51eZ+UBLn0uA6cDzO/Ddus3nge9n5k9aGx3zcXEwcF1EfDvKYdn+iDh6cKFjPi6uAg6M\niOcCRMTas1AVAAAGs0lEQVQelMf7/KCad8zHUReO70uAKzLz8bY+u0XE9NF8t40+5FD2RmwCLGlr\nX0L5Q9cQIoZ8mOo2lL/Mw43lDOCx6h/Q2vpsQ9l9+ReZuZISplr7DPU50LA/t4h4M7AncMIQix3z\nztuF8pvkYuCVlF30n4uIv6+WO+ad9yngW8DNEfEY5b5mp2fmN6vljvn46rbx7difQTfcDFAT02gf\npqoxiIhnUsLkQZm5ou56NhKTgGsz88Rq/oaIeAHlxqXfqK+sRjsMeAvwZuAmSqg/IyLuzkzHXGPm\nnpxy3G8lJaW2mgHcu+HL6X5RnjU2F3h5Zt7TsuheyvlMw43lvcDUiNhihD7tZ+xvAjytrc9QnwPN\n+nPrAZ4O9EfEiohYQTnp773Vb7xLcMw77R5gUVvbIsoJseDf8/HwaeBTmfmdzLwxM88HTmP13kvH\nfHx12/h27M9gow851W/HC4EDB9uqQzEH0mXP4OgGsfphqgdk28NUM/MOyl/A1rHcgnIsdnAsF1JO\nQmvtsxvlB8jVVdPVwJYR8aKWzR9I+Uf4Py19do+IrVv6vBIYoPwm2BSXUa5A2BPYo5quA84D9sjM\n23HMO+1nlBMqW+0G/Ab8ez5Onkz5ZbPVKqqfUY75+OrC8b0aeFkVkFr7LM7MgdF+uY1+olye9ghr\nXkL+B+DpddfWTRPlENVDwBxKqh6cNm3pc3w1dgdTfjhfBPyaNS9DPJtyie7LKXsqfsYTL0P8AeWH\n+Ysph8QWA99oWT4JuIFyueILKWfwLwE+Vvc4bYA/h/arqxzzzo7vXpSrSE4Ank05jPJH4M2O+biN\n+bmUE1jnUi7Zfy3l3I5THPOOjfHmlF+S9qQEyPdV8zt02/hSruS6m3IJ+SzK4cw/AW8f9feue+C7\nZQLeTbn+/1FKityr7pq6bar+YawcYnpbW7+PVH9BH6GcEf+ctuVPAs6kHCr8I/Ad4Bltfbak7K0Y\noASrrwBPbuuzA+VePX+q/pH8KzCp7nHaAH8OP6El5Djm4zLGc4FfVuN5I3DUEH0c886N9+bAqZQf\noMsoP1xPpu2Sbcd8vcZ4f4b+P/yr3Ti+lHv1/LSq5S7gn8fyvX1ApyRJaqSN/pwcSZLUTIYcSZLU\nSIYcSZLUSIYcSZLUSIYcSZLUSIYcSZLUSIYcSZLUSIYcSZLUSIYcSWMWEZdHxKl119EqIlZFxCF1\n1yGpft7xWNKYRcSWwIrMXBYRdwCnZebnNtBnnwS8JjNf1Nb+DOChLA/flbQRm1x3AZImrsx8uNPb\njIgpowgoT/gtLTPv63BJkiYoD1dJGrPqcNVpEXE55enRp1WHi1a29HlpRFwREY9ExG8i4oyIeHLL\n8jsi4kMR8bWIGAC+VLV/KiIWR8SyiLgtIj4aEZtUy44ATgL2GPy8iHhbtWyNw1UR8YKI+HH1+Q9E\nxJciYvOW5edGxIUR8U8RcXfV56zBz5I0cRlyJK2vBF4L/A44EdgG2BYgIp4N/JDytOIXAIcB+1Ge\nZNzqn4DrgT2Bj1VtS4G3ATOB9wBHA/OrZd8CPkt5QviM6vO+1V5YFaYuAf4A9ABvAA4a4vMPAHYB\nXl595pHVJGkC83CVpPWWmQ9Xe2/+1Ha46IPAeZk5GCpuj4j3AQsi4l2Z+VjV/uPMPK1tm6e0zN4V\nEZ+lhKT/k5nLI+JPwOOZef8wpR0OPAl4W2YuBxZFxLHA9yPiAy3rPggcm+UkxVsi4mLgQODfRjsW\nkrqHIUfSeNoD2D0i3trSFtXrzsDi6v3C9hUj4jDgH4FnA9Mo/18NjPLznwfcUAWcQT+j7MXeDRgM\nOTfmmldh3EPZ8yRpAjPkSBpP0yjn2JzB6nAz6K6W98taF0TES4DzKIe/LqWEm17guHGqs/1E58TD\n+dKEZ8iR1CmPAe0n6/YDszLzjlFua1/gzsz81GBDROy0Dp/XbhFwRERslpmPVm0vBVayei+SpIby\nNxVJnXIn8LKI2C4itqra/hXYNyLOjIg9IuI5EXFoRLSf+Nvu18CzIuKwiNglIt4DvGaIz9u52u5W\nETF1iO2cDywHvhYRz4+IA4DPAV8f4VweSQ1gyJG0PlrPY/kwsBNwG3AfQGb+CtgfeC5wBWXPzkeA\n369lG1TrfR84jXIV1C+AlwAfbev2PeC/gcurz3tz+/aqvTevAp4GXAt8G/gR5VwfSQ3nHY8lSVIj\nuSdHkiQ1kiFHkiQ1kiFHkiQ1kiFHkiQ1kiFHkiQ1kiFHkiQ1kiFHkiQ1kiFHkiQ1kiFHkiQ1kiFH\nkiQ1kiFHkiQ1kiFHkiQ10v8HCQL3/zAK6Z0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5f6d59aa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Desent ||Norm||= 1.5  RMS: 3.8767587135713364\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def mainClosedFormSol(dataset):\n",
    "    \n",
    "    tdsPhi=dataset[0];\n",
    "    tdsY=dataset[1];\n",
    "    vdsPhi=dataset[2];\n",
    "    vdsY=dataset[3];\n",
    "    ttds=dataset[4];\n",
    "    #--------------------[Closed Form Sol without Regularlization]--------------------------------\n",
    "    #Find w*\n",
    "    wStar=trainUsingClosedFormEquation(tdsPhi,tdsY);\n",
    "    #Predict y* for Validate Data\n",
    "    ystar=pridict(vdsPhi,wStar);\n",
    "    #checking for RMS for Validate Data\n",
    "    rms=getRMS(vdsY,ystar);\n",
    "    #Predict y* for TestData\n",
    "    ystar=pridict(ttds,wStar);\n",
    "    writeTestData(ystar);\n",
    "    print(\"Closed Form Solution RMS:\",rms);\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    pass;\n",
    "\n",
    "\n",
    "def mainRidgeClosedFormSol(dataset,lam):\n",
    "    #-------------------------------------\n",
    "    # Best value: m=300 validate=120\n",
    "    #-------------------------------------    \n",
    "    tdsPhi=dataset[0];\n",
    "    tdsY=dataset[1];\n",
    "    vdsPhi=dataset[2];\n",
    "    vdsY=dataset[3];\n",
    "    ttds=dataset[4];\n",
    "    #--------------------[Closed Form Sol without Regularlization]--------------------------------\n",
    "    #Find w*\n",
    "    wStar=trainUsingClosedFormRidgeEq(tdsPhi,tdsY,lam=lam);\n",
    "    print(wStar);\n",
    "    ystar=pridict(tdsPhi,wStar);\n",
    "    rms=getRMS(tdsY,ystar);\n",
    "    print(\"Closed FormSol With Trained Data Ridge RMS \",rms);\n",
    "    \n",
    "    #Predict y* for Validate Data\n",
    "    ystar=pridict(vdsPhi,wStar);\n",
    "    #checking for RMS for Validate Data\n",
    "    rms=getRMS(vdsY,ystar);\n",
    "    #Predict y* for TestData\n",
    "    ystar=pridict(ttds,wStar);\n",
    "    writeTestData(ystar);\n",
    "    print(\"Closed FormSol With validate Ridge RMS:\",rms);\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    pass;\n",
    "\n",
    "def mainGradientDesent(dataset,lamdaVal=0.0001,alphaVal=0.0003,maxIter=100000,algFixedIter=True):\n",
    "    tdsPhi=dataset[0];\n",
    "    tdsY=dataset[1];\n",
    "    vdsPhi=dataset[2];\n",
    "    vdsY=dataset[3];\n",
    "    ttds=dataset[4];\n",
    "    #--------------------[Gradient decent with Regularlization]--------------------------------\n",
    "    wStar=gardientDescentWithRidge(tdsPhi,tdsY,maxIteration=maxIter,algFixedIteration=algFixedIter,lam=lamdaVal,alpha=alphaVal);\n",
    "    #Predict y* for Validate Data\n",
    "    ystar=pridict(vdsPhi,wStar);\n",
    "    #checking for RMS for Validate Data\n",
    "    rms=getRMS(vdsY,ystar);\n",
    "    #Predict y* for TestData\n",
    "    ystar=pridict(ttds,wStar);\n",
    "    writeTestData(ystar);\n",
    "    print(\"Gradient Desent ||Norm||=2 RMS:\",rms);\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "def mainGradientDesentLpnorm(dataset,pnorm,lamdaVal=0.0001,alphaVal=0.0003,maxIter=100000,algFixedIter=True):\n",
    "    tdsPhi=dataset[0];\n",
    "    tdsY=dataset[1];\n",
    "    vdsPhi=dataset[2];\n",
    "    vdsY=dataset[3];\n",
    "    ttds=dataset[4];\n",
    "    #--------------------[Gradient decent with Regularlization]--------------------------------\n",
    "    wStar=gardientDescentWithPnom(tdsPhi,tdsY,pnorm,maxIteration=maxIter,algFixedIteration=algFixedIter,lam=lamdaVal,alpha=alphaVal);\n",
    "    #Predict y* for Validate Data\n",
    "    ystar=pridict(vdsPhi,wStar);\n",
    "    #checking for RMS for Validate Data\n",
    "    rms=getRMS(vdsY,ystar);\n",
    "    #Predict y* for TestData\n",
    "    ystar=pridict(ttds,wStar);\n",
    "    writeTestData(ystar);\n",
    "    print(\"Gradient Desent ||Norm||=\",pnorm,\" RMS:\",rms);\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    \n",
    "#split train data into Train and Validate\n",
    "def spitTrainDataset(phi,y,tdSize=300):\n",
    "    m=len(phi);        \n",
    "    tdsSize=int(m*trainDSSizePercentage);\n",
    "    l=tdSize;    \n",
    "    trainDatasetPhi=phi[0:l];\n",
    "    trainDatasetY=y[0:l];\n",
    "    validateDatasetPhi=phi[tdsSize:m];\n",
    "    validateDatasetY=y[tdsSize:m];    \n",
    "   \n",
    "    return [trainDatasetPhi,trainDatasetY,validateDatasetPhi,validateDatasetY];    \n",
    "    pass\n",
    "\n",
    "def getRoughNewAlpha(currentAlpha,currentRMS,oldRMS,iteration):\n",
    "    newAlpha=currentAlpha;\n",
    "    rate=10;\n",
    "    if(currentAlpha<1):\n",
    "        if(iteration%100==0 and currentRMS>=100):\n",
    "            #decrease the learning rate\n",
    "            newAlpha=newAlpha/rate;\n",
    "        elif(iteration%100==0 and currentRMS<=100):\n",
    "            #increase the learning rate\n",
    "            newAlpha=newAlpha*rate;            \n",
    "    return newAlpha;\n",
    "    pass;\n",
    "\n",
    "def costFunction(phi,y,w,lamda):\n",
    "    phi=np.array(phi);\n",
    "    y=np.array(y);\n",
    "    w=np.array(w);\n",
    "    phiW=np.dot(phi,w);\n",
    "    phiWminusY=phiW-y;\n",
    "    phiWminusYSq=np.power(np.linalg.norm(phiWminusY),2);\n",
    "    wSq=np.power(np.linalg.norm(w),2);\n",
    "    cost=phiWminusYSq+(lamda*wSq);\n",
    "    return cost;\n",
    "    pass;\n",
    "\n",
    "def gardientCostFunction(phi,y,w,lamda):\n",
    "    phi=np.array(phi);\n",
    "    y=(np.array(y));#converting row vector to col vector    \n",
    "    phiT=np.transpose(phi);\n",
    "    phiTphi=np.dot(phiT,phi);   \n",
    "    phiTy=np.dot(phiT,y);   \n",
    "    wDash=(np.dot(phiTphi,w)-phiTy)+(lamda*w);   \n",
    "    return wDash;\n",
    "    pass;\n",
    "\n",
    "def getNewAlphaByBacktrack(phi,y,w,lamda):\n",
    "    #f(x − deltaf(x))    \n",
    "    beta=0.2;    \n",
    "    t=1;    \n",
    "    while(True):\n",
    "        gw=gardientCostFunction(phi,y,w,lamda);\n",
    "        y1=costFunction(phi,y,(w-gw),lamda);\n",
    "        y3=costFunction(phi,y,w,lamda);\n",
    "        y4=np.power(np.linalg.norm(gw),2);\n",
    "        y2=y3-(t*0.5*y4);\n",
    "        print(\"t=\",t,\" y1=\",y1,\" y2=\",y2,\" y3=\",y3,\" y4=\",y4);\n",
    "        if(y1<=y2):\n",
    "            break;\n",
    "        t=beta*t;\n",
    "    print(\"T==\",t);\n",
    "    return t;\n",
    "    pass;\n",
    " \n",
    "# GD: Least Sq. With Ridges\n",
    "def gardientDescentWithRidge(phi,y,lam,alpha,wi=-1,maxIteration=100000,algFixedIteration=True):\n",
    "    m=len(y);#no of data points\n",
    "    n=len(phi[0]);# no. of features    \n",
    "    #alpha=.000000000003; #learning parameter\n",
    "    #maxIteration=100000;\n",
    "    phi=np.array(phi);\n",
    "    y=(np.array(y));#converting row vector to col vector    \n",
    "    if(wi==-1):\n",
    "        wk0=np.zeros(n);# Nx1 vector\n",
    "    else:\n",
    "        wk0=phi[wi]\n",
    "    #.................................................................................\n",
    "    # Re-intilatinzing w0 after interration for fast convergence. As with un normalized datas it takes many hours\n",
    "    # to converge. Below are the hours\n",
    "    #1. Iteration completed: approx 10^7 Note: closed in between as taking too many hours\n",
    "    #No of Features: 122. #9\n",
    "    #wk0=[0.00000024,  0.00000012,  0.0000102 , -0.00000061,  0.00000011,  0.00000011,  0.00000255,  0.00000992,  0.00000046,  0.00000182,  0.00004097,  0.00000237,  0.00001784, -0.00000492,  0.00001923,  0.00059716, -0.0000254 ,  0.00000011,  0.00000005,  0.00002387,  0.0004208 ,  0.00000008,  0.00001407,  0.00000225,  0.00001286,  0.00005349, -0.00015837,  0.00000013,  0.00000154,  0.00000016,  0.00000011,  0.00000016,  0.0000008 ,  0.0000015 ,  0.00000034,  0.00000066,  0.00000325,  0.00000077,  0.00000189, -0.0000004 ,  0.00000017,  0.00000062,  0.00000023,  0.00000011,  0.00000019,  0.00000044,  0.00000059,  0.00000029,  0.0000004 ,  0.00000088,  0.00000043,  0.00000064,  0.00000009,  0.0000002,  0.0000004 ,  0.00000024,  0.00000011,  0.00000021,  0.00000032,  0.00000037,  0.00000026,  0.00000031,  0.00000046,  0.00000032,  0.00000038,  0.00000019,  0.00000021,  0.00000032,  0.00000024,  0.00000011,  0.00000022,  0.00000028,  0.0000003 ,  0.00000025,  0.00000027,  0.00000033,  0.00000027,  0.0000003,  0.00000022,  0.00000022,  0.00000029,  0.00000024,  0.00000011,  0.00000023,  0.00000025,  0.00000026,  0.00000024,  0.00000025,  0.00000028,  0.00000025,  0.00000026,  0.00000023,  0.00000103,  0.00000044, -0.00000009,  0.00000208,  0.00000516,  0.00000017,  0.00001691,  0.00000044, -0.00000017,  0.00002003, -0.00001763, -0.00000042,  0.00000008,  0.00000029,  0.00000091,  0.00000007,  0.00000018,  0.00002817,  0.0000021 ,  0.00001942, -0.0000175 , -0.00002501, -0.00034786, -0.00009305,  0.00000288,  0.00002624, -0.00000867, -0.0000018,  0.00005376, -0.00024956];\n",
    "    #2. Iteration completed: approx 30^7\n",
    "    #No of Features: 122. #9\n",
    "    #wk0=[0.00000228,  0.00000166,  0.0000588 , -0.00000667,  0.00000112,  0.00000105 , 0.00002484,  0.00007256,  0.00000481,  0.00002081,  0.00038317,  0.00002285 , 0.00017698, -0.00004922,  0.00016757,  0.00169232, -0.00028316,  0.00000112 , 0.00000046,  0.00023441,  0.00079649,  0.00000846,  0.00020069,  0.00000013 , 0.0001202 ,  0.00001994, -0.00150773,  0.00000141,  0.00001167,  0.00000154 , 0.00000112,  0.00000155,  0.00000773,  0.00001297,  0.00000335,  0.000007 , 0.00003096,  0.00000745,  0.00001856, -0.00000423,  0.00000174,  0.00000525 , 0.00000224,  0.00000112,  0.00000188,  0.00000423,  0.00000542,  0.00000277 , 0.00000404,  0.00000846,  0.00000414,  0.00000621,  0.00000075,  0.00000197 , 0.00000353,  0.00000231,  0.00000112,  0.00000207,  0.00000311,  0.0000035 , 0.00000251,  0.00000304,  0.0000044 ,  0.00000308,  0.0000037 ,  0.00000177 , 0.00000211,  0.00000289,  0.0000023 ,  0.00000112,  0.00000217,  0.00000267 , 0.00000283,  0.00000239,  0.00000263,  0.00000317,  0.00000264,  0.00000289 , 0.00000208,  0.00000218,  0.00000262,  0.00000229,  0.00000112,  0.00000222 , 0.00000246,  0.00000253,  0.00000233,  0.00000245,  0.00000269,  0.00000245 , 0.00000256,  0.00000219,  0.00001796,  0.00000484, -0.00000063,  0.00002088 , 0.00006706,  0.00000127,  0.00030351,  0.00000484, -0.00000153,  0.00018922 ,-0.00002858, -0.00000523,  0.00000083,  0.00000306,  0.00000947,  0.00000074 , 0.00000186,  0.00026123,  0.00003142,  0.00030906, -0.00016762, -0.00005072 ,-0.00060271, -0.00081242,  0.00002939,  0.00026672, -0.00008577,  0.00002142 , 0.0000694 , -0.00239048]\n",
    "    #3. Iteration completed: 10^8 Note: closed in between as taking too much time   \n",
    "    #No of Features: 122. #9\n",
    "    #wk0=[ 0.00000429,  0.00000303,  0.00009374, -0.00001128,  0.0000021 ,  0.000002 , 0.00004604,  0.00013154,  0.00000918,  0.00003969,  0.00071764,  0.00004357 , 0.00033577, -0.0000826 ,  0.00016853,  0.0016491 , -0.00052001,  0.0000021 , 0.00000089,  0.00043243,  0.00090792,  0.00001753,  0.00039021, -0.0000005 , 0.000243  ,  0.00002083, -0.00245923,  0.00000285,  0.00002031,  0.0000032 , 0.0000021 ,  0.00000294,  0.00001441,  0.00002406,  0.00000633,  0.00001328 , 0.00005819,  0.0000141 ,  0.00003515, -0.0000067 ,  0.00000338,  0.00000939 , 0.00000432,  0.0000021 ,  0.00000355,  0.00000792,  0.00001013,  0.00000522 , 0.00000763,  0.00001592,  0.00000782,  0.00001174,  0.00000173,  0.00000377 , 0.00000638,  0.00000439,  0.0000021 ,  0.0000039 ,  0.00000584,  0.00000658 , 0.00000473,  0.00000574,  0.00000828,  0.00000581,  0.00000699,  0.00000345 , 0.00000401,  0.00000525,  0.00000435,  0.0000021 ,  0.00000409,  0.00000502 , 0.00000532,  0.0000045 ,  0.00000496,  0.00000597,  0.00000498,  0.00000545 , 0.00000397,  0.00000413,  0.00000477,  0.00000433,  0.0000021 ,  0.00000419 , 0.00000463,  0.00000477,  0.00000439,  0.00000462,  0.00000506,  0.00000462 , 0.00000483,  0.00000415,  0.00003196,  0.00000905, -0.00000106,  0.00003276 , 0.00012011,  0.00000159,  0.00053434,  0.00000905, -0.00000267,  0.00028351 ,-0.00003651, -0.00001183,  0.00000155,  0.00000575,  0.00001774,  0.00000138 , 0.00000346,  0.00042074,  0.00006398,  0.00062391, -0.0002836 , -0.00004993 ,-0.00059305, -0.00137195,  0.00005671,  0.00050825, -0.00014477,  0.00007982 , 0.00006566, -0.00391314]\n",
    "    #4. Iteration completed: approx 10^9 Note: closed in between as taking too much time       \n",
    "    #No of Features: 122. #9\n",
    "    #wk0=[0.00004052,  0.00002065,  0.00069759, -0.0000394 ,  0.00001962,  0.00001968 , 0.00040321,  0.0011131 ,  0.00008939,  0.00034313,  0.00663027,  0.00041162 , 0.00312193, -0.00028812, -0.0023957 ,  0.00140897, -0.00346996,  0.00001962 , 0.00000968,  0.0036987 ,  0.00112531,  0.00018129,  0.00298163, -0.00001466 , 0.002241  ,  0.00005348, -0.00486676,  0.00003018,  0.0001699 ,  0.00004229 , 0.00001962,  0.00002819,  0.00012959,  0.00021572,  0.00006005,  0.00012034 , 0.00054421,  0.00013331,  0.00032923, -0.00000214,  0.0000337 ,  0.00008128 , 0.00004461,  0.00001962,  0.00003378,  0.00007276,  0.00009321,  0.00004923 , 0.00007059,  0.00014967,  0.00007393,  0.00011056,  0.00003193,  0.00003653 , 0.00005598,  0.00004297,  0.00001962,  0.00003699,  0.00005436,  0.00006136 , 0.00004463,  0.00005368,  0.00007801,  0.00005482,  0.00006592,  0.00003824 , 0.00003835,  0.00004641,  0.00004181,  0.00001962,  0.00003871,  0.00004696 , 0.00004985,  0.00004251,  0.00004668,  0.00005625,  0.00004713,  0.00005145 , 0.0000398 ,  0.00003937,  0.00004226,  0.00004119,  0.00001962,  0.00003961 , 0.00004361,  0.00004493,  0.0000415 ,  0.00004351,  0.00004774,  0.0000437 , 0.0000456 ,  0.00004025,  0.000164  ,  0.00007975, -0.00000602,  0.00004818 , 0.00073583, -0.00003249,  0.00257965,  0.00007975, -0.00001627, -0.00006224 ,-0.00009306, -0.00025038,  0.00001433,  0.00005328,  0.00016427,  0.00001233 , 0.00003093,  0.00062573,  0.00062125,  0.00668248, -0.00113086, -0.00049438 ,-0.00063504, -0.005849  ,  0.00053172,  0.00476664, -0.00054677,  0.00043788 , 0.00000347, -0.00788869]\n",
    "    #5. Iteration completed: approx 10^11 Note: closed in between as taking too much time       \n",
    "    #No of Features: 122. #9\n",
    "    wk0=[42.8612625  ,  -1.14046757 ,  -1.06252978 ,   0.46944066 ,  -5.60391846 , -70.3726374  ,  80.96000485 ,   1.46490995 , -21.7970572  ,   8.78716327 ,   0.55415016 , -10.34289432 ,   0.31692929 ,  -7.5507631  ,   0.00550376 ,   0.00257729 ,  -0.01818881 ,  -5.63320282 ,  28.32398072 ,  -0.61417676 ,  -0.00260213 ,   0.64126723 ,  -0.09233672 ,  -0.00019618 ,   0.26894577 ,  -0.00044879 ,   0.05783857 ,  -1.72766649 ,  25.18052798 ,  10.13240115 ,  -5.66807777 ,  -3.16682888 ,-294.52215742 , -34.4466717  , 158.72466719 , -71.68585093 , -17.36430966 ,  48.96856689 , -12.49998749 ,  12.72021565 ,  -6.23757794 , -74.29890105 , -43.93825806 ,  -5.66794087 ,  32.83971965 ,-141.54238961 ,  61.41514829 ,-159.23631207 ,  65.81724008 ,   3.99525975 ,  51.39600836 ,  73.98036484 , -54.17796862 ,  21.61591299 , -12.43479129 ,  -3.54078479 ,  -5.66893607 ,  43.47934744 , -47.22248303 ,  58.00643945 ,-142.24547841 ,  75.4571528  ,  30.84068789 ,  48.82509585 , -63.79548858 , -12.0651105  ,  -5.94995138 ,  26.46394656 ,  21.61376516 ,  -5.6684142 ,  45.56536849 ,  -0.44191492 ,  51.7966019  , -71.45755854 ,  64.80971002 ,  39.54059307 ,  46.97318198 , -53.47229276 ,  16.13392281 ,   0.33558013 ,  46.16207529 ,  33.71502413 ,  -5.6684142  ,  45.59435143 ,  22.44114991 ,  48.336994   , -19.12355243 ,  55.9343373  ,  42.60482153 ,  45.94498624 , -16.66883355 ,  30.60821494 ,  -0.00972615 ,   0.72039779 , -16.94477838 ,   1.26481702 ,   0.0464093  ,  -0.65534518 ,   0.00675854 ,   0.7203978 ,  12.50531302 ,  -0.12152569 ,  -0.00027075 ,   0.17945797 ,  90.85927153 ,   2.13092033 ,  -1.22866057 , -57.09407799 ,   8.47001552 ,  -0.00176077 ,   3.40268353 ,  -0.07203281 ,  -1.95184606 ,  -0.10104856 ,   0.00181168 ,   0.04610832 ,  -9.30557176 ,   0.06615403 ,   7.41262044 ,  -0.02141117 ,   0.00021472 ,  -0.03124094]\n",
    "    wk0=np.array(wk0);\n",
    "    phiT=np.transpose(phi);\n",
    "    phiTphi=np.dot(phiT,phi);   \n",
    "    phiTy=np.dot(phiT,y);   \n",
    "    alphaBym=alpha;\n",
    "    xaxis=list();\n",
    "    yaxis=list();\n",
    "    #algFixedIteration=True;\n",
    "    logReading=True;\n",
    "    diff=0;\n",
    "    #-----------------------------------------------------------------\n",
    "    #Best Tested Constant\n",
    "    #1)With 13 features i.e original Phi\n",
    "    #normalize phi: true.\n",
    "    #aplha=.212 lamda=.301 tds=300 vds=120 trained o/p=4.8310 rms\n",
    "    #Note: Tried for different initial wk0 from phi matrix but o/p remain same\n",
    "    #\n",
    "    #2)With 26 features i.e newphi=phi + phi^2\n",
    "    #normalize phi: true; algFixedIteration=True; maxIteration=1000000;\n",
    "    #aplha=.0003 lamda=.301 tds=300 vds=120 trained o/p=3.940800315632070 rms\n",
    "    #Tried for different initial wk0 but o/p remain same\n",
    "    #-----------------------------------------------------------------\n",
    "    print(\"Training Started (Least Sq. With Ridge) ...\");\n",
    "    oldRms=0;\n",
    "    currentAlpha=alphaBym;\n",
    "    if (algFixedIteration):\n",
    "        for iteration in range(0,maxIteration):  \n",
    "            wk1=wk0-(currentAlpha*((np.dot(phiTphi,wk0)-phiTy)+(lam*wk0)));              \n",
    "            ystar=pridict(phi,wk1);\n",
    "            rms=getRMS(y,ystar);    \n",
    "            xaxis.append(iteration);\n",
    "            yaxis.append(rms);\n",
    "            percentComplete=((iteration+1)*100)/maxIteration;\n",
    "            if(percentComplete%10==0 ):\n",
    "                print(\"#alpha\",currentAlpha,\"Percent Completed\",percentComplete,\"rms:\",rms);                \n",
    "            wk0=wk1;    \n",
    "            oldRms=rms;\n",
    "    else:\n",
    "        diffOffset=1e-12;\n",
    "        iteration=0;\n",
    "        oldRms=0;\n",
    "        voldRms=0;\n",
    "        while (True):\n",
    "            wk1=wk0-(alphaBym*((np.dot(phiTphi,wk0)-phiTy)+(lam*wk0)));                     \n",
    "            ystar=pridict(phi,wk1);\n",
    "            rms=getRMS(y,ystar);    \n",
    "            xaxis.append(iteration);\n",
    "            yaxis.append(rms);\n",
    "            diff=abs(oldRms-rms);            \n",
    "            if(iteration>0 and diff<diffOffset):\n",
    "                break;\n",
    "            if(False and iteration%100==0 ):\n",
    "                print(\"# iteration: \",iteration,\" rms:\",rms,\"diff:\",diff);            \n",
    "            if(iteration%100000==0):\n",
    "                print(wk1);\n",
    "            wk0=wk1;\n",
    "            oldRms=rms;\n",
    "            iteration+=1;\n",
    "        print(\"# iteration: \",iteration,\" rms:\",rms,\"diff:\",diff);    \n",
    "           \n",
    "    print(\"Final Trained RMS:\",rms ,\". Iteration needed \", iteration);    \n",
    "    #-------------------------------------------------------------\n",
    "    if(logReading):\n",
    "        writeReadingInFile(\"ridge1.csv\",alpha,lam,iteration,rms,2);\n",
    "    plotGraph(xaxis,yaxis);\n",
    "    print(wk1);\n",
    "    return wk1;\n",
    "\n",
    "\n",
    "# GD: Least Sq. With ||w||_(1.5)^(1.5)\n",
    "def gardientDescentWithPnom(phi,y,p,lam,alpha,maxIteration=100000,algFixedIteration=True):\n",
    "    m=len(y);#no of data points\n",
    "    n=len(phi[0]);# no. of features    \n",
    "    #alpha=0.0003 #learning parameter\n",
    "    #maxIteration=1000000;\n",
    "    phi=np.array(phi);\n",
    "    y=(np.array(y));#converting row vector to col vector    \n",
    "    wk0=np.zeros(n);# Nx1 vector    \n",
    "    #wk0=phi[1];\n",
    "    #Taken from L2 norm values\n",
    "    #5. Iteration completed: approx 10^11 Note: closed in between as taking too much time       \n",
    "    #No of Features: 122. #9\n",
    "    wk0=[42.8612625  ,  -1.14046757 ,  -1.06252978 ,   0.46944066 ,  -5.60391846 , -70.3726374  ,  80.96000485 ,   1.46490995 , -21.7970572  ,   8.78716327 ,   0.55415016 , -10.34289432 ,   0.31692929 ,  -7.5507631  ,   0.00550376 ,   0.00257729 ,  -0.01818881 ,  -5.63320282 ,  28.32398072 ,  -0.61417676 ,  -0.00260213 ,   0.64126723 ,  -0.09233672 ,  -0.00019618 ,   0.26894577 ,  -0.00044879 ,   0.05783857 ,  -1.72766649 ,  25.18052798 ,  10.13240115 ,  -5.66807777 ,  -3.16682888 ,-294.52215742 , -34.4466717  , 158.72466719 , -71.68585093 , -17.36430966 ,  48.96856689 , -12.49998749 ,  12.72021565 ,  -6.23757794 , -74.29890105 , -43.93825806 ,  -5.66794087 ,  32.83971965 ,-141.54238961 ,  61.41514829 ,-159.23631207 ,  65.81724008 ,   3.99525975 ,  51.39600836 ,  73.98036484 , -54.17796862 ,  21.61591299 , -12.43479129 ,  -3.54078479 ,  -5.66893607 ,  43.47934744 , -47.22248303 ,  58.00643945 ,-142.24547841 ,  75.4571528  ,  30.84068789 ,  48.82509585 , -63.79548858 , -12.0651105  ,  -5.94995138 ,  26.46394656 ,  21.61376516 ,  -5.6684142 ,  45.56536849 ,  -0.44191492 ,  51.7966019  , -71.45755854 ,  64.80971002 ,  39.54059307 ,  46.97318198 , -53.47229276 ,  16.13392281 ,   0.33558013 ,  46.16207529 ,  33.71502413 ,  -5.6684142  ,  45.59435143 ,  22.44114991 ,  48.336994   , -19.12355243 ,  55.9343373  ,  42.60482153 ,  45.94498624 , -16.66883355 ,  30.60821494 ,  -0.00972615 ,   0.72039779 , -16.94477838 ,   1.26481702 ,   0.0464093  ,  -0.65534518 ,   0.00675854 ,   0.7203978 ,  12.50531302 ,  -0.12152569 ,  -0.00027075 ,   0.17945797 ,  90.85927153 ,   2.13092033 ,  -1.22866057 , -57.09407799 ,   8.47001552 ,  -0.00176077 ,   3.40268353 ,  -0.07203281 ,  -1.95184606 ,  -0.10104856 ,   0.00181168 ,   0.04610832 ,  -9.30557176 ,   0.06615403 ,   7.41262044 ,  -0.02141117 ,   0.00021472 ,  -0.03124094]\n",
    "    wk0=np.array(wk0);    \n",
    "    phiT=np.transpose(phi);\n",
    "    phiTphi=np.dot(phiT,phi);   \n",
    "    phiTy=np.dot(phiT,y);   \n",
    "    alphaBym=alpha/m;\n",
    "    #lam=0.31;\n",
    "    xaxis=list();\n",
    "    yaxis=list();\n",
    "    #algFixedIteration=True;\n",
    "    logReading=True;\n",
    "    diff=0;\n",
    "    wPow=p-1;\n",
    "    if (p<=1):\n",
    "        print(\"Error: norm p is less than 1 i.p p=\",wPow);\n",
    "        return None;\n",
    "        \n",
    "    #-----------------------------------------------------------------\n",
    "    print(\"Training Started (Least Sq. With Ridge) ...\");\n",
    "    if (algFixedIteration):\n",
    "        for iteration in range(0,maxIteration):\n",
    "            if (wPow>1):\n",
    "                wk0Pow=np.power(wk0,wPow);            \n",
    "            else:\n",
    "                wk0Pow=wk0;\n",
    "            wk1=wk0-(alphaBym*((np.dot(phiTphi,wk0)-phiTy)+(lam*wk0Pow))); \n",
    "            \n",
    "            ystar=pridict(phi,wk1);\n",
    "            rms=getRMS(y,ystar);    \n",
    "            xaxis.append(iteration);\n",
    "            yaxis.append(rms);\n",
    "            percentComplete=((iteration+1)*100)/maxIteration;\n",
    "            if( percentComplete%10==0 ):\n",
    "                print(\"Percent Completed\",percentComplete,\" rms:\",rms);\n",
    "            wk0=wk1;\n",
    "    else:\n",
    "        diffOffset=1e-20;\n",
    "        iteration=0;\n",
    "        oldRms=0;\n",
    "        voldRms=0;\n",
    "        while (True):            \n",
    "            if (wPow>1):\n",
    "                wk0Pow=np.power(wk0,wPow);            \n",
    "            else:\n",
    "                wk0Pow=wk0;\n",
    "            wk1=wk0-(alphaBym*((np.dot(phiTphi,wk0)-phiTy)+(lam*wk0Pow)));  \n",
    "            ystar=pridict(phi,wk1);\n",
    "            rms=getRMS(y,ystar);    \n",
    "            xaxis.append(iteration);\n",
    "            yaxis.append(rms);\n",
    "            diff=abs(oldRms-rms);                      \n",
    "            \n",
    "            if(iteration>0 and  diff<=diffOffset):\n",
    "                break;\n",
    "            if(False and iteration%100==0 ):\n",
    "                print(\"# iteration: \",iteration,\" rms:\",rms,\"diff:\",diff);            \n",
    "            wk0=wk1;\n",
    "            oldRms=rms;            \n",
    "            iteration+=1;\n",
    "        print(\"# iteration: \",iteration,\" rms:\",rms,\"diff:\",diff);    \n",
    "\n",
    "    print(\"Final Trained RMS:\",rms ,\". Iteration needed \", iteration);       \n",
    "    #-------------------------------------------------------------\n",
    "    if(logReading):\n",
    "        writeReadingInFile(\"pnom.csv\",alpha,lam,iteration,rms,p);\n",
    "    plotGraph(xaxis,yaxis);\n",
    "    return wk1; \n",
    "\n",
    "# Finding w*=(QTQ+lamI)^-1QTY\n",
    "def trainUsingClosedFormRidgeEq(dataset,output,lam=0.0001):\n",
    "    #-------------------------------------\n",
    "    # 1)Best value: m=300 validate=120\n",
    "    # addW0=True: c=0.376654. Not lamd\n",
    "    # Note: Please make eq as phiT_phi+c\n",
    "    #\n",
    "    # 2)Best value: m=300 validate=120\n",
    "    # addW0=True: lam=0.521. Not C. \n",
    "    # trained RMS:  3.938231279881311\n",
    "    # Note: Please make eq as phiT_phi+lamI\n",
    "    #-------------------------------------\n",
    "    m=len(dataset);\n",
    "    n=len(dataset[0]);    \n",
    "    phi=np.array(dataset);\n",
    "    y=np.array(output);\n",
    "    phiT=np.transpose(phi);    \n",
    "    #(QTQ)    \n",
    "    phiT_phi=np.dot(phiT,phi);\n",
    "    n=len(phiT_phi);    \n",
    "    c=.301;\n",
    "    I=np.identity(n);\n",
    "    lamI=lam*I;\n",
    "    d=getDet(phiT_phi)\n",
    "    #--------------------------------------\n",
    "    if(True or d>0):\n",
    "        #(QTQ+lamI)^-1\n",
    "        phiT_phi_inv=inv((phiT_phi+lamI));\n",
    "        #(QTQ+lamI)^-1QT\n",
    "        phiT_phi_inv_phiT=np.dot(phiT_phi_inv,phiT);  \n",
    "        #(QTQ+lamI)^-1QT*Y\n",
    "        w=np.dot(phiT_phi_inv_phiT,y);\n",
    "        return w;\n",
    "    else:\n",
    "        print(\"Error:Phi is NOT full column rank.\");\n",
    "        return None;\n",
    "    pass;\n",
    "\n",
    "\n",
    "\n",
    "def getKernalFeature(phi,wrtdataPoint):    \n",
    "    p=np.array(phi);           \n",
    "    wrtdata=np.array(phiSet[wrtdataPoint]);     \n",
    "    tempMatrix=p-wrtdata;    \n",
    "    fcol=np.sum(tempMatrix,axis=1);\n",
    "    sigma=2;\n",
    "    fcol=np.power(fcol,2);\n",
    "    fcol=fcol*(-1*(1/sigma));\n",
    "    fcol=np.exp(fcol);    \n",
    "    return fcol;\n",
    "    pass;\n",
    "\n",
    "\n",
    "def createNewFeatureMatrix(phi,maxDeg=4):    \n",
    "    newPhi=np.array(phi);\n",
    "    p=np.array(phi);        \n",
    "    if(addW0Col):#deleting first W0Col coffecient is 1\n",
    "        p=np.delete(p,0,1);    \n",
    "\n",
    "    TotalColumn=len(p[0]);\n",
    "    HalfColumn=int(len(p[0])/2)+1;  \n",
    "    \n",
    "    #---------------------------------------------------------------------------------\n",
    "    #Best Features till now.\n",
    "    #1. feature sq2 with lamda 0.001\n",
    "    #2. feature sq2 + sq root with lamda 0.001\n",
    "    #3. feature sq2 + 1/2 root + 1/4 root with lamda 0.001 rms: 3.02849\n",
    "    #\n",
    "    #4. feature sq2 + 1/2 root + 1/4 root + 1/8 + 1/16  + 1/32 lamda=0.0001\n",
    "    #\n",
    "    #5. feature sq2 + 1/2 root + 1/4 root + 1/8 + 1/16 +  + 1/32 [x0.x1 x0.x2 x0.x3....x0.x7] lamda=0.0001\n",
    "    #\n",
    "    #6. feature sq2 + 1/2 root + 1/4 root + 1/8 + 1/16 +  + 1/32 [x0.x1 x0.x2 ....x0.x7]+ [x0.x1^2 x0.x2^2 ..x0.x7^2] lamda=0.0001\n",
    "    #   train rms= 2.732538945291264 validate Rms=4.711844963854281 test RMS=3.014\n",
    "    #\n",
    "    #7. feature sq2 + 1/2 root + 1/4 root + 1/8 + 1/16 +  + 1/32 [x0.x1 x0.x2 ....] +[x0.x1^2 x0.x2^2...x0.x7^2] \n",
    "    #   [x3.x4^0.05 x3.x5^0.05]\n",
    "    #   lamda=0.0001 train rms= 2.6523470519008407 validate Rms=4.490001110608855 test RMS=2.85636\\\n",
    "    #\n",
    "    #8. feature sq2 + 1/2 root + 1/4 root + 1/8 + 1/16 +  + 1/32 [x0.x1 x0.x1 ....x0.x7] +[x0.x1^2 x0.x2^2..x0.x7^2] \n",
    "    #   + [x3.x4^0.5 x3.x5^0.5 .... x3.x7^0.5]\n",
    "    #\n",
    "    #9. feature sq2 + 1/2 root + 1/4 root + 1/8 + 1/16 +  + 1/32 [x0.x1 x0.x1 ....x0.x7] +[x0.x1^2 x0.x2^2..x0.x7^2] \n",
    "    #   + [x3.x4^0.5 x3.x5^0.5...x3.x7^0.5] + [x3.x4^2.x5^0.4 x3.x4^2.x6^0.4 x3.x4^2.x7^0.4]     \n",
    "    #   lamda=0.0001 train rms=2.6147993385918764 validate Rms=2.6147993385918764 test rms=2.7\n",
    "    #\n",
    "    #10. feature sq2 + 1/2 root + 1/4 root + 1/8 + 1/16 + 1/32 [x0.x1 x0.x1 ...x0.x7] +[x0.x1^2 x0.x2^2 ...x0.x7^2] \n",
    "    #   + [x3.x4^0.5 x3.x5^0.5 ... x3.x7^0.5] + [x3.x4^2.x5^0.4 x3.x4^2.x6^0.4 x3.x4^2.x7^0.4]     \n",
    "    #   + [x7.x10 .... x7.x13]\n",
    "    #   lamda=0.0001 train rms=2.5057404497731555 validate Rms=4.230169910269796 test rms=2.55676    \n",
    "    #\n",
    "    #11. feature sq2 + 1/2 root + 1/4 root + 1/8 + 1/16 + 1/32 [x0.x1 x0.x1 ...x0.x7] +[x0.x1^2 x0.x2^2 ...x0.x7^2] \n",
    "    #   + [x3.x4^0.5 x3.x5^0.5 ... x3.x7^0.5] + [x3.x4^2.x5^0.4 x3.x4^2.x6^0.4 x3.x4^2.x7^0.4]     \n",
    "    #   + [x7.x10 .... x7.x13] + [x7.x10^2 .... x7.x13^2] + [x7.x10^0.5 .... x7.x13^0.5]\n",
    "    #   lamda=0.0001 train rms=2.340500337718980 validate Rms=3.8389206328449315 test rms=2.35368    \n",
    "    # \n",
    "    #--------------------------------------------------------------------------------------------\n",
    "   \n",
    "    absP=np.abs(p)\n",
    "    pPowI=np.power(p,2);        \n",
    "    newPhi=np.concatenate((newPhi,pPowI),axis=1);     \n",
    "        \n",
    "    pPowI=np.power(absP,0.5);        \n",
    "    newPhi=np.concatenate((newPhi,pPowI),axis=1); \n",
    "        \n",
    "    pPowI=np.power(absP,0.25);        \n",
    "    newPhi=np.concatenate((newPhi,pPowI),axis=1); \n",
    "   \n",
    "    pPowI=np.power(absP,0.125);        \n",
    "    newPhi=np.concatenate((newPhi,pPowI),axis=1); \n",
    "       \n",
    "    pPowI=np.power(absP,0.0625);        \n",
    "    newPhi=np.concatenate((newPhi,pPowI),axis=1); \n",
    "        \n",
    "    pPowI=np.power(absP,0.03125);        \n",
    "    newPhi=np.concatenate((newPhi,pPowI),axis=1); \n",
    " \n",
    "    c1=p[:,0];        \n",
    "    for i in range(1,HalfColumn):\n",
    "        c2=p[:,i+1];\n",
    "        c3=c1*c2;\n",
    "        newPhi=np.insert(newPhi,len(newPhi[0]),c3,axis=1);    \n",
    "    \n",
    "    \n",
    "    c1=p[:,0];            \n",
    "    pPowI=np.power(p,2);                \n",
    "    for i in range(1,HalfColumn):\n",
    "        c2=pPowI[:,i+1];\n",
    "        c3=c1*c2;\n",
    "        newPhi=np.insert(newPhi,len(newPhi[0]),c3,axis=1);    \n",
    "    \n",
    "    \n",
    "    c1=p[:,3];                \n",
    "    pPowI=np.power(absP,0.5);                \n",
    "    for i in range(4,HalfColumn):\n",
    "        c2=pPowI[:,i];\n",
    "        c3=c1*c2;\n",
    "        newPhi=np.insert(newPhi,len(newPhi[0]),c3,axis=1);    \n",
    "    \n",
    "    c1=p[:,3];      \n",
    "    c2=p[:,4]**2;\n",
    "    n=int(len(p[0])/2)+1;                    \n",
    "    pPowI=np.power(absP,0.4);                \n",
    "    for i in range(5,HalfColumn):\n",
    "        c3=pPowI[:,i];\n",
    "        c=c1*c2*c3;\n",
    "        newPhi=np.insert(newPhi,len(newPhi[0]),c,axis=1);    \n",
    "    \n",
    "    n=int(len(p[0])/2)+1;                 \n",
    "    c1=p[:,7];      \n",
    "    for i in range(9,TotalColumn):\n",
    "        c3=p[:,i];\n",
    "        c=c3*c1;\n",
    "        newPhi=np.insert(newPhi,len(newPhi[0]),c,axis=1);    \n",
    "       \n",
    "    c1=np.abs(p[:,7])**2;\n",
    "    for i in range(10,TotalColumn):\n",
    "        c3=p[:,i];\n",
    "        c=c3*c1;\n",
    "        newPhi=np.insert(newPhi,len(newPhi[0]),c,axis=1);    \n",
    "    \n",
    "    c1=np.abs(p[:,7])**0.5;\n",
    "    for i in range(10,TotalColumn):\n",
    "        c3=p[:,i];\n",
    "        c=c3*c1;\n",
    "        newPhi=np.insert(newPhi,len(newPhi[0]),c,axis=1);    \n",
    "    \n",
    "    c1=np.abs(p[:,7])**0.5;\n",
    "    pPowI=np.power(absP,2);                   \n",
    "    for i in range(10,TotalColumn):\n",
    "        c3=pPowI[:,i];\n",
    "        c=c3*c1;\n",
    "        newPhi=np.insert(newPhi,len(newPhi[0]),c,axis=1);           \n",
    "    \n",
    "    return newPhi;    \n",
    "    pass;\n",
    "\n",
    "#--settings--\n",
    "np.set_printoptions(suppress=True)\n",
    "#---init---\n",
    "dir=\"\"\n",
    "trainFile=dir+\"train.csv\";\n",
    "testFile=dir+\"test.csv\";\n",
    "trainDSSizePercentage=0.7; # x*100 percentage. 1-x data set will be used for validating\n",
    "addW0Col=True;\n",
    "tdSize=280;\n",
    "#---------------------------------------------\n",
    "print(\"Fetching Trained Dataset from file...\");\n",
    "dataset=readTrainData(trainFile);\n",
    "testDS=readTestData(testFile);\n",
    "phiSet=dataset[0];\n",
    "ySet=dataset[1];\n",
    "\n",
    "newPhiSet=createNewFeatureMatrix(phiSet,2);\n",
    "newTestDS=createNewFeatureMatrix(testDS,2);\n",
    "\n",
    "\n",
    "phiSet_norm=normalizePhi(newPhiSet);\n",
    "testDS_norm=normalizePhi(newTestDS);\n",
    "\n",
    "tds=spitTrainDataset(phiSet,ySet,tdSize);\n",
    "tds_norm=spitTrainDataset(phiSet_norm,ySet,tdSize);\n",
    "\n",
    "print(\"Fetching of data Completed.\");\n",
    "\n",
    "#train set\n",
    "trainDatasetPhi=tds[0];\n",
    "trainDatasetY=tds[1];\n",
    "validateDatasetPhi=tds[2];\n",
    "validateDatasetY=tds[3];\n",
    "gtdPhi=trainDatasetPhi;\n",
    "\n",
    "trainDatasetPhi_norm=tds_norm[0];\n",
    "trainDatasetY_norm=tds_norm[1];\n",
    "validateDatasetPhi_norm=tds_norm[2];\n",
    "validateDatasetY_norm=tds_norm[3];\n",
    "\n",
    "\n",
    "deg=2;\n",
    "#trainDatasetPhi_norm=createNewFeatureMatrix(trainDatasetPhi_norm,deg);\n",
    "#validateDatasetPhi_norm=createNewFeatureMatrix(validateDatasetPhi_norm,deg);\n",
    "#testDS_norm=createNewFeatureMatrix(testDS_norm,deg);\n",
    "\n",
    "deg=2;\n",
    "trainDatasetPhi=createNewFeatureMatrix(trainDatasetPhi,deg);\n",
    "validateDatasetPhi=createNewFeatureMatrix(validateDatasetPhi,deg);\n",
    "testDS=createNewFeatureMatrix(testDS,deg);\n",
    "\n",
    "print(\"Train Size:\"+str(len(trainDatasetPhi)));\n",
    "print(\"Validate Size:\"+str(len(validateDatasetPhi)));\n",
    "\n",
    "ds=[trainDatasetPhi,trainDatasetY,validateDatasetPhi,validateDatasetY,testDS];\n",
    "ds_norm=[trainDatasetPhi_norm,trainDatasetY,validateDatasetPhi_norm,validateDatasetY,testDS_norm];\n",
    "#mainClosedFormSol(ds);\n",
    "mainRidgeClosedFormSol(ds,0.0001);\n",
    "#mainGradientDesent(ds,lamdaVal=0.0001,alphaVal=0.000000000000001,maxIter=1000000);\n",
    "#mainGradientDesentLpnorm(ds,3/2,lamdaVal=0.0001,alphaVal=0.00000000001,maxIter=100000);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
