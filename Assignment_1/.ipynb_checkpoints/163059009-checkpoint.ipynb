{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "from numpy.linalg import inv;\n",
    "from numpy.linalg import det;\n",
    "import math;\n",
    "\n",
    "# Will read the file and convert it into two dataset one train data other validate data\n",
    "def readTrainData(fileName):\n",
    "    row_index=0;\n",
    "    phi=list();\n",
    "    y=list();\n",
    "    with open(fileName) as f:\n",
    "        for line in f:\n",
    "            if row_index >0:\n",
    "                phi_i=list((float(n) for n in line.split('\\n')[0].split(\",\") ));\n",
    "                if(addW0Col):\n",
    "                    phi_i[0]=1;\n",
    "                else:# removing id col.\n",
    "                    phi_i.pop();\n",
    "                # last row is value of yi                \n",
    "                y_i=phi_i.pop(len(phi_i)-1); \n",
    "                phi.append(phi_i);             \n",
    "                y.append(y_i);\n",
    "            row_index+=1;\n",
    "    return [phi,y];\n",
    "#End-readTrainData\n",
    "\n",
    "# Will read the file and convert it into dataset for Testing the Model\n",
    "def readTestData(fileName):\n",
    "    row_index=0;\n",
    "    phi=list();\n",
    "    y=list();\n",
    "    with open(fileName) as f:\n",
    "        for line in f:\n",
    "            if row_index >0:                \n",
    "                phi_i=list((float(n) for n in line.split('\\n')[0].split(\",\") ));\n",
    "                if(addW0Col):\n",
    "                    phi_i[0]=1;\n",
    "                else:# removing id col.\n",
    "                    phi_i.pop();                \n",
    "                phi.append(phi_i);                             \n",
    "            row_index+=1;\n",
    "    m=len(phi);    \n",
    "    return phi;\n",
    "#End-readTrainData\n",
    "\n",
    "\n",
    "\n",
    "#write-output\n",
    "def writeTestData(ystar,outputFile=\"output.csv\"):\n",
    "    fo = open(outputFile, \"w\");    \n",
    "    fo.write(\"ID,MEDV\\n\");\n",
    "    m=len(ystar);\n",
    "    for i in range(m):\n",
    "        fo.write(str(i)+\",\"+str(ystar[i])+\"\\n\");\n",
    "    fo.close();\n",
    "    pass;\n",
    "\n",
    "# Return det of matrix\n",
    "def getDet(A):\n",
    "    d=det(A);\n",
    "    if(d<10**-10):\n",
    "        return 0;\n",
    "    return d;\n",
    "\n",
    "\n",
    "#Return RMS: root mean square error\n",
    "def getRMS(y,yStar):\n",
    "    m=len(y);\n",
    "    sigma=0;\n",
    "    for i in range(m):\n",
    "        delta=(y[i]-yStar[i]);\n",
    "        delta=delta*delta;\n",
    "        sigma=sigma+delta;\n",
    "    meanSq=sigma/m;   \n",
    "    rms=math.sqrt(meanSq);\n",
    "    return rms;\n",
    "    pass;\n",
    "\n",
    "#For ploting graph of RMS VS Iteration\n",
    "def plotGraph(x,y):\n",
    "    import matplotlib.pyplot as plt;\n",
    "    plt.plot(x,y)\n",
    "    plt.ylabel('rms')\n",
    "    plt.xlabel('iteration');\n",
    "    plt.show();\n",
    "    pass;\n",
    "\n",
    "#Record readings for gradient descent\n",
    "def writeReadingInFile(filename,alpha,lam,iteration,rms,p):\n",
    "    import os.path;\n",
    "    import datetime;\n",
    "    import time;\n",
    "    ts = datetime.datetime.fromtimestamp(time.time()).strftime('%d-%m-%Y %H:%M:%S')\n",
    "    if(os.path.exists(filename)==False):\n",
    "        fo = open(filename, \"w\"); \n",
    "        fo.write(\"iteration,norm,alpha,lam,rms,timestamp\\n\");\n",
    "        fo.write(str(iteration)+\",\"+str(p)+\",\"+str(alpha)+\",\"+str(lam)+\",\"+str(rms)+\",\"+str(ts)+\"\\n\");\n",
    "    else:\n",
    "        fo = open(filename, \"a\"); \n",
    "        fo.write(str(iteration)+\",\"+str(p)+\",\"+str(alpha)+\",\"+str(lam)+\",\"+str(rms)+\",\"+str(ts)+\"\\n\");\n",
    "    fo.close();                    \n",
    "    pass;\n",
    "\n",
    "\n",
    "#normalize the data set ny (x-u)/s where s is max-min\n",
    "def normalizePhi(unNormalizedPhi):    \n",
    "    phi=np.array(unNormalizedPhi);\n",
    "    print(\"Normalizing Phi...\");  \n",
    "    std=phi.std(0);\n",
    "    mean=phi.mean(0); \n",
    "    if(addW0Col):#making first col. mean as 0              \n",
    "        std[0]=1;\n",
    "        mean[0]=0;\n",
    "    phi_normalize=(phi-mean)/std;    \n",
    "    print(\"Normalization done.\");\n",
    "    return phi_normalize;\n",
    "    pass;\n",
    "\n",
    "#pridict of y* given w* QW=y*\n",
    "def pridict(dataset,weight):\n",
    "    phi=np.array(dataset);\n",
    "    w=np.array(weight);\n",
    "    ystar=np.dot(phi,w);\n",
    "    return ystar;\n",
    "    pass;\n",
    "\n",
    "# Finding w*=(QTQ)^-1QTY\n",
    "def trainUsingClosedFormEquation(dataset,output):\n",
    "    m=len(dataset);\n",
    "    n=len(dataset[0]);\n",
    "    phi=np.array(dataset);\n",
    "    y=np.array(output);\n",
    "    phiT=np.transpose(phi);\n",
    "    #(QTQ)    \n",
    "    phiT_phi=np.dot(phiT,phi); \n",
    "    d=getDet(phiT_phi)\n",
    "    if(True or d>0):\n",
    "        #(QTQ)^-1\n",
    "        phiT_phi_inv=inv(phiT_phi);\n",
    "        #(QTQ)^-1QT\n",
    "        phiT_phi_inv_phiT=np.dot(phiT_phi_inv,phiT);  \n",
    "        #(QTQ)^-1QT*Y\n",
    "        w=np.dot(phiT_phi_inv_phiT,y);      \n",
    "        return w;   \n",
    "    else:\n",
    "        print(\"Error:Phi is NOT full column rank.\");\n",
    "        return None;\n",
    "    pass;\n",
    "\n",
    "def numpiTestFun():\n",
    "    A2= np.matrix([[4,6],[2,8]])        \n",
    "    A3= np.matrix([[1,2,3],[4,5,7],[7,8,9]])\n",
    "    A=A2;\n",
    "    print(A);\n",
    "    print(np.power(A,0.5));\n",
    "    print(A);\n",
    "    print(\"Det(A):\"+str(getDet(A)));\n",
    "    B= np.transpose(A);\n",
    "    C=inv(A);\n",
    "    #print(C);\n",
    "    print(np.dot(A,C));\n",
    "    print(A.std(0));\n",
    "    print(A.mean(0));\n",
    "    print(normalizePhi(A));\n",
    "    norm=(A-A.mean(0))/A.std(0);    \n",
    "    print(norm);    \n",
    "    print();\n",
    "    pass;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GD: Least Sq. Without Regularlization\n",
    "def gardientDescentErrorFun(phi,y):\n",
    "    m=len(y);#no of data points\n",
    "    n=len(phi[0]);# no. of features    \n",
    "    alpha=0.22;# learning parameter\n",
    "    maxIteration=10000;\n",
    "    phi=np.array(phi);\n",
    "    y=(np.array(y));#converting row vector to col vector    \n",
    "    wk0=np.zeros(n);# Nx1 vector\n",
    "    phiT=np.transpose(phi);\n",
    "    phiTphi=np.dot(phiT,phi);   \n",
    "    phiTy=np.dot(phiT,y);   \n",
    "    alphaBym=alpha/m;\n",
    "    xaxis=list();\n",
    "    yaxis=list();\n",
    "    #----------------------\n",
    "    print(\"Training Started (Least Sq. Without Regularlization) ...\");\n",
    "    for i in range(maxIteration):  \n",
    "        wk1=wk0-(alphaBym*((np.dot(phiTphi,wk0)-phiTy)));                \n",
    "        ystar=pridict(phi,wk1);\n",
    "        rms=getRMS(y,ystar);    \n",
    "        xaxis.append(i);\n",
    "        yaxis.append(rms);\n",
    "        percentComplete=((i+1)*100)/maxIteration;\n",
    "        if( percentComplete%10==0 ):\n",
    "            print(\"Percent Completed\",percentComplete);\n",
    "        wk0=wk1;\n",
    "    print(\"Final Trained RMS:\",rms);\n",
    "    plotGraph(xaxis,yaxis);\n",
    "    return wk1;\n",
    "    pass;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Trained Dataset from file...\n",
      "Normalizing Phi...\n",
      "Normalization done.\n",
      "Normalizing Phi...\n",
      "Normalization done.\n",
      "Fetching of data Completed.\n",
      "Train Size:280\n",
      "Validate Size:120\n",
      "Closed FormSol With Trained Data Ridge RMS  2.2900944479699508\n",
      "Closed FormSol With validate Ridge RMS: 4.119217570116554\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def mainClosedFormSol(dataset):\n",
    "    \n",
    "    tdsPhi=dataset[0];\n",
    "    tdsY=dataset[1];\n",
    "    vdsPhi=dataset[2];\n",
    "    vdsY=dataset[3];\n",
    "    ttds=dataset[4];\n",
    "    #--------------------[Closed Form Sol without Regularlization]--------------------------------\n",
    "    #Find w*\n",
    "    wStar=trainUsingClosedFormEquation(tdsPhi,tdsY);\n",
    "    #Predict y* for Validate Data\n",
    "    ystar=pridict(vdsPhi,wStar);\n",
    "    #checking for RMS for Validate Data\n",
    "    rms=getRMS(vdsY,ystar);\n",
    "    #Predict y* for TestData\n",
    "    ystar=pridict(ttds,wStar);\n",
    "    writeTestData(ystar);\n",
    "    print(\"Closed Form Solution RMS:\",rms);\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    pass;\n",
    "\n",
    "\n",
    "def mainRidgeClosedFormSol(dataset,lam):\n",
    "    #-------------------------------------\n",
    "    # Best value: m=300 validate=120\n",
    "    #-------------------------------------    \n",
    "    tdsPhi=dataset[0];\n",
    "    tdsY=dataset[1];\n",
    "    vdsPhi=dataset[2];\n",
    "    vdsY=dataset[3];\n",
    "    ttds=dataset[4];\n",
    "    #--------------------[Closed Form Sol without Regularlization]--------------------------------\n",
    "    #Find w*\n",
    "    wStar=trainUsingClosedFormRidgeEq(tdsPhi,tdsY,lam=lam);\n",
    "    ystar=pridict(tdsPhi,wStar);\n",
    "    rms=getRMS(tdsY,ystar);\n",
    "    print(\"Closed FormSol With Trained Data Ridge RMS \",rms);\n",
    "    \n",
    "    #Predict y* for Validate Data\n",
    "    ystar=pridict(vdsPhi,wStar);\n",
    "    #checking for RMS for Validate Data\n",
    "    rms=getRMS(vdsY,ystar);\n",
    "    #Predict y* for TestData\n",
    "    ystar=pridict(ttds,wStar);\n",
    "    writeTestData(ystar);\n",
    "    print(\"Closed FormSol With validate Ridge RMS:\",rms);\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    pass;\n",
    "\n",
    "def mainGradientDesent(dataset,lamdaVal=0.0001,alphaVal=0.0003,maxIter=100000,algFixedIter=True):\n",
    "    tdsPhi=dataset[0];\n",
    "    tdsY=dataset[1];\n",
    "    vdsPhi=dataset[2];\n",
    "    vdsY=dataset[3];\n",
    "    ttds=dataset[4];\n",
    "    #--------------------[Gradient decent with Regularlization]--------------------------------\n",
    "    wStar=gardientDescentWithRidge(tdsPhi,tdsY,maxIteration=maxIter,algFixedIteration=algFixedIter,lam=lamdaVal,alpha=alphaVal);\n",
    "    #Predict y* for Validate Data\n",
    "    ystar=pridict(vdsPhi,wStar);\n",
    "    #checking for RMS for Validate Data\n",
    "    rms=getRMS(vdsY,ystar);\n",
    "    #Predict y* for TestData\n",
    "    ystar=pridict(ttds,wStar);\n",
    "    writeTestData(ystar,\"output.csv\");\n",
    "    print(\"Gradient Desent ||Norm||=2 RMS:\",rms);\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "def mainGradientDesentLpnorm(dataset,pnorm,outputFile,lamdaVal=0.0001,alphaVal=0.0003,maxIter=100000,algFixedIter=True):\n",
    "    tdsPhi=dataset[0];\n",
    "    tdsY=dataset[1];\n",
    "    vdsPhi=dataset[2];\n",
    "    vdsY=dataset[3];\n",
    "    ttds=dataset[4];\n",
    "    #--------------------[Gradient decent with Regularlization]--------------------------------\n",
    "    wStar=gardientDescentWithPnom(tdsPhi,tdsY,pnorm,maxIteration=maxIter,algFixedIteration=algFixedIter,lam=lamdaVal,alpha=alphaVal);\n",
    "    #Predict y* for Validate Data\n",
    "    ystar=pridict(vdsPhi,wStar);\n",
    "    #checking for RMS for Validate Data\n",
    "    rms=getRMS(vdsY,ystar);\n",
    "    #Predict y* for TestData\n",
    "    ystar=pridict(ttds,wStar);\n",
    "    writeTestData(ystar,outputFile);\n",
    "    print(\"Gradient Desent ||Norm||=\",pnorm,\" RMS:\",rms);\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    \n",
    "#split train data into Train and Validate\n",
    "def spitTrainDataset(phi,y,tdSize=300):\n",
    "    m=len(phi);        \n",
    "    tdsSize=int(m*trainDSSizePercentage);\n",
    "    l=tdSize;    \n",
    "    trainDatasetPhi=phi[0:l];\n",
    "    trainDatasetY=y[0:l];\n",
    "    validateDatasetPhi=phi[tdsSize:m];\n",
    "    validateDatasetY=y[tdsSize:m];    \n",
    "   \n",
    "    return [trainDatasetPhi,trainDatasetY,validateDatasetPhi,validateDatasetY];    \n",
    "    pass\n",
    "\n",
    "def getRoughNewAlpha(currentAlpha,currentRMS,oldRMS,iteration):\n",
    "    newAlpha=currentAlpha;\n",
    "    rate=10;\n",
    "    if(currentAlpha<1):\n",
    "        if(iteration%100==0 and currentRMS>=100):\n",
    "            #decrease the learning rate\n",
    "            newAlpha=newAlpha/rate;\n",
    "        elif(iteration%100==0 and currentRMS<=100):\n",
    "            #increase the learning rate\n",
    "            newAlpha=newAlpha*rate;            \n",
    "    return newAlpha;\n",
    "    pass;\n",
    "\n",
    "def costFunction(phi,y,w,lamda):\n",
    "    phi=np.array(phi);\n",
    "    y=np.array(y);\n",
    "    w=np.array(w);\n",
    "    phiW=np.dot(phi,w);\n",
    "    phiWminusY=phiW-y;\n",
    "    phiWminusYSq=np.power(np.linalg.norm(phiWminusY),2);\n",
    "    wSq=np.power(np.linalg.norm(w),2);\n",
    "    cost=phiWminusYSq+(lamda*wSq);\n",
    "    return cost;\n",
    "    pass;\n",
    "\n",
    "def gardientCostFunction(phi,y,w,lamda):\n",
    "    phi=np.array(phi);\n",
    "    y=(np.array(y));#converting row vector to col vector    \n",
    "    phiT=np.transpose(phi);\n",
    "    phiTphi=np.dot(phiT,phi);   \n",
    "    phiTy=np.dot(phiT,y);   \n",
    "    wDash=(np.dot(phiTphi,w)-phiTy)+(lamda*w);   \n",
    "    return wDash;\n",
    "    pass;\n",
    "\n",
    "def getNewAlphaByBacktrack(phi,y,w,lamda):\n",
    "    #f(x − deltaf(x))    \n",
    "    beta=0.2;    \n",
    "    t=1;    \n",
    "    while(True):\n",
    "        gw=gardientCostFunction(phi,y,w,lamda);\n",
    "        y1=costFunction(phi,y,(w-gw),lamda);\n",
    "        y3=costFunction(phi,y,w,lamda);\n",
    "        y4=np.power(np.linalg.norm(gw),2);\n",
    "        y2=y3-(t*0.5*y4);\n",
    "        print(\"t=\",t,\" y1=\",y1,\" y2=\",y2,\" y3=\",y3,\" y4=\",y4);\n",
    "        if(y1<=y2):\n",
    "            break;\n",
    "        t=beta*t;\n",
    "    print(\"T==\",t);\n",
    "    return t;\n",
    "    pass;\n",
    " \n",
    "# GD: Least Sq. With Ridges\n",
    "def gardientDescentWithRidge(phi,y,lam,alpha,wi=-1,maxIteration=100000,algFixedIteration=True):\n",
    "    m=len(y);#no of data points\n",
    "    n=len(phi[0]);# no. of features    \n",
    "    #alpha=.000000000003; #learning parameter\n",
    "    #maxIteration=100000;\n",
    "    phi=np.array(phi);\n",
    "    y=(np.array(y));#converting row vector to col vector    \n",
    "    if(wi==-1):\n",
    "        wk0=np.zeros(n);# Nx1 vector\n",
    "    else:\n",
    "        wk0=phi[wi]\n",
    "    #.................................................................................\n",
    "    # Re-intilatinzing w0 after interration for fast convergence. As with un normalized datas it takes many hours\n",
    "    # to converge. Below are the hours\n",
    "    #1. Iteration completed: approx 10^7 Note: closed in between as taking too many hours\n",
    "    #No of Features: 122. #9\n",
    "    #wk0=[0.00000024,  0.00000012,  0.0000102 , -0.00000061,  0.00000011,  0.00000011,  0.00000255,  0.00000992,  0.00000046,  0.00000182,  0.00004097,  0.00000237,  0.00001784, -0.00000492,  0.00001923,  0.00059716, -0.0000254 ,  0.00000011,  0.00000005,  0.00002387,  0.0004208 ,  0.00000008,  0.00001407,  0.00000225,  0.00001286,  0.00005349, -0.00015837,  0.00000013,  0.00000154,  0.00000016,  0.00000011,  0.00000016,  0.0000008 ,  0.0000015 ,  0.00000034,  0.00000066,  0.00000325,  0.00000077,  0.00000189, -0.0000004 ,  0.00000017,  0.00000062,  0.00000023,  0.00000011,  0.00000019,  0.00000044,  0.00000059,  0.00000029,  0.0000004 ,  0.00000088,  0.00000043,  0.00000064,  0.00000009,  0.0000002,  0.0000004 ,  0.00000024,  0.00000011,  0.00000021,  0.00000032,  0.00000037,  0.00000026,  0.00000031,  0.00000046,  0.00000032,  0.00000038,  0.00000019,  0.00000021,  0.00000032,  0.00000024,  0.00000011,  0.00000022,  0.00000028,  0.0000003 ,  0.00000025,  0.00000027,  0.00000033,  0.00000027,  0.0000003,  0.00000022,  0.00000022,  0.00000029,  0.00000024,  0.00000011,  0.00000023,  0.00000025,  0.00000026,  0.00000024,  0.00000025,  0.00000028,  0.00000025,  0.00000026,  0.00000023,  0.00000103,  0.00000044, -0.00000009,  0.00000208,  0.00000516,  0.00000017,  0.00001691,  0.00000044, -0.00000017,  0.00002003, -0.00001763, -0.00000042,  0.00000008,  0.00000029,  0.00000091,  0.00000007,  0.00000018,  0.00002817,  0.0000021 ,  0.00001942, -0.0000175 , -0.00002501, -0.00034786, -0.00009305,  0.00000288,  0.00002624, -0.00000867, -0.0000018,  0.00005376, -0.00024956];\n",
    "    #2. Iteration completed: approx 30^7\n",
    "    #No of Features: 122. #9\n",
    "    #wk0=[0.00000228,  0.00000166,  0.0000588 , -0.00000667,  0.00000112,  0.00000105 , 0.00002484,  0.00007256,  0.00000481,  0.00002081,  0.00038317,  0.00002285 , 0.00017698, -0.00004922,  0.00016757,  0.00169232, -0.00028316,  0.00000112 , 0.00000046,  0.00023441,  0.00079649,  0.00000846,  0.00020069,  0.00000013 , 0.0001202 ,  0.00001994, -0.00150773,  0.00000141,  0.00001167,  0.00000154 , 0.00000112,  0.00000155,  0.00000773,  0.00001297,  0.00000335,  0.000007 , 0.00003096,  0.00000745,  0.00001856, -0.00000423,  0.00000174,  0.00000525 , 0.00000224,  0.00000112,  0.00000188,  0.00000423,  0.00000542,  0.00000277 , 0.00000404,  0.00000846,  0.00000414,  0.00000621,  0.00000075,  0.00000197 , 0.00000353,  0.00000231,  0.00000112,  0.00000207,  0.00000311,  0.0000035 , 0.00000251,  0.00000304,  0.0000044 ,  0.00000308,  0.0000037 ,  0.00000177 , 0.00000211,  0.00000289,  0.0000023 ,  0.00000112,  0.00000217,  0.00000267 , 0.00000283,  0.00000239,  0.00000263,  0.00000317,  0.00000264,  0.00000289 , 0.00000208,  0.00000218,  0.00000262,  0.00000229,  0.00000112,  0.00000222 , 0.00000246,  0.00000253,  0.00000233,  0.00000245,  0.00000269,  0.00000245 , 0.00000256,  0.00000219,  0.00001796,  0.00000484, -0.00000063,  0.00002088 , 0.00006706,  0.00000127,  0.00030351,  0.00000484, -0.00000153,  0.00018922 ,-0.00002858, -0.00000523,  0.00000083,  0.00000306,  0.00000947,  0.00000074 , 0.00000186,  0.00026123,  0.00003142,  0.00030906, -0.00016762, -0.00005072 ,-0.00060271, -0.00081242,  0.00002939,  0.00026672, -0.00008577,  0.00002142 , 0.0000694 , -0.00239048]\n",
    "    #3. Iteration completed: 10^8 Note: closed in between as taking too much time   \n",
    "    #No of Features: 122. #9\n",
    "    #wk0=[ 0.00000429,  0.00000303,  0.00009374, -0.00001128,  0.0000021 ,  0.000002 , 0.00004604,  0.00013154,  0.00000918,  0.00003969,  0.00071764,  0.00004357 , 0.00033577, -0.0000826 ,  0.00016853,  0.0016491 , -0.00052001,  0.0000021 , 0.00000089,  0.00043243,  0.00090792,  0.00001753,  0.00039021, -0.0000005 , 0.000243  ,  0.00002083, -0.00245923,  0.00000285,  0.00002031,  0.0000032 , 0.0000021 ,  0.00000294,  0.00001441,  0.00002406,  0.00000633,  0.00001328 , 0.00005819,  0.0000141 ,  0.00003515, -0.0000067 ,  0.00000338,  0.00000939 , 0.00000432,  0.0000021 ,  0.00000355,  0.00000792,  0.00001013,  0.00000522 , 0.00000763,  0.00001592,  0.00000782,  0.00001174,  0.00000173,  0.00000377 , 0.00000638,  0.00000439,  0.0000021 ,  0.0000039 ,  0.00000584,  0.00000658 , 0.00000473,  0.00000574,  0.00000828,  0.00000581,  0.00000699,  0.00000345 , 0.00000401,  0.00000525,  0.00000435,  0.0000021 ,  0.00000409,  0.00000502 , 0.00000532,  0.0000045 ,  0.00000496,  0.00000597,  0.00000498,  0.00000545 , 0.00000397,  0.00000413,  0.00000477,  0.00000433,  0.0000021 ,  0.00000419 , 0.00000463,  0.00000477,  0.00000439,  0.00000462,  0.00000506,  0.00000462 , 0.00000483,  0.00000415,  0.00003196,  0.00000905, -0.00000106,  0.00003276 , 0.00012011,  0.00000159,  0.00053434,  0.00000905, -0.00000267,  0.00028351 ,-0.00003651, -0.00001183,  0.00000155,  0.00000575,  0.00001774,  0.00000138 , 0.00000346,  0.00042074,  0.00006398,  0.00062391, -0.0002836 , -0.00004993 ,-0.00059305, -0.00137195,  0.00005671,  0.00050825, -0.00014477,  0.00007982 , 0.00006566, -0.00391314]\n",
    "    #4. Iteration completed: approx 10^9 Note: closed in between as taking too much time       \n",
    "    #No of Features: 122. #9\n",
    "    #wk0=[0.00004052,  0.00002065,  0.00069759, -0.0000394 ,  0.00001962,  0.00001968 , 0.00040321,  0.0011131 ,  0.00008939,  0.00034313,  0.00663027,  0.00041162 , 0.00312193, -0.00028812, -0.0023957 ,  0.00140897, -0.00346996,  0.00001962 , 0.00000968,  0.0036987 ,  0.00112531,  0.00018129,  0.00298163, -0.00001466 , 0.002241  ,  0.00005348, -0.00486676,  0.00003018,  0.0001699 ,  0.00004229 , 0.00001962,  0.00002819,  0.00012959,  0.00021572,  0.00006005,  0.00012034 , 0.00054421,  0.00013331,  0.00032923, -0.00000214,  0.0000337 ,  0.00008128 , 0.00004461,  0.00001962,  0.00003378,  0.00007276,  0.00009321,  0.00004923 , 0.00007059,  0.00014967,  0.00007393,  0.00011056,  0.00003193,  0.00003653 , 0.00005598,  0.00004297,  0.00001962,  0.00003699,  0.00005436,  0.00006136 , 0.00004463,  0.00005368,  0.00007801,  0.00005482,  0.00006592,  0.00003824 , 0.00003835,  0.00004641,  0.00004181,  0.00001962,  0.00003871,  0.00004696 , 0.00004985,  0.00004251,  0.00004668,  0.00005625,  0.00004713,  0.00005145 , 0.0000398 ,  0.00003937,  0.00004226,  0.00004119,  0.00001962,  0.00003961 , 0.00004361,  0.00004493,  0.0000415 ,  0.00004351,  0.00004774,  0.0000437 , 0.0000456 ,  0.00004025,  0.000164  ,  0.00007975, -0.00000602,  0.00004818 , 0.00073583, -0.00003249,  0.00257965,  0.00007975, -0.00001627, -0.00006224 ,-0.00009306, -0.00025038,  0.00001433,  0.00005328,  0.00016427,  0.00001233 , 0.00003093,  0.00062573,  0.00062125,  0.00668248, -0.00113086, -0.00049438 ,-0.00063504, -0.005849  ,  0.00053172,  0.00476664, -0.00054677,  0.00043788 , 0.00000347, -0.00788869]\n",
    "    #5. Iteration completed: approx 10^11 Note: closed in between as taking too much time       \n",
    "    #No of Features: 122. #9\n",
    "    wk0=[42.8612625  ,  -1.14046757 ,  -1.06252978 ,   0.46944066 ,  -5.60391846 , -70.3726374  ,  80.96000485 ,   1.46490995 , -21.7970572  ,   8.78716327 ,   0.55415016 , -10.34289432 ,   0.31692929 ,  -7.5507631  ,   0.00550376 ,   0.00257729 ,  -0.01818881 ,  -5.63320282 ,  28.32398072 ,  -0.61417676 ,  -0.00260213 ,   0.64126723 ,  -0.09233672 ,  -0.00019618 ,   0.26894577 ,  -0.00044879 ,   0.05783857 ,  -1.72766649 ,  25.18052798 ,  10.13240115 ,  -5.66807777 ,  -3.16682888 ,-294.52215742 , -34.4466717  , 158.72466719 , -71.68585093 , -17.36430966 ,  48.96856689 , -12.49998749 ,  12.72021565 ,  -6.23757794 , -74.29890105 , -43.93825806 ,  -5.66794087 ,  32.83971965 ,-141.54238961 ,  61.41514829 ,-159.23631207 ,  65.81724008 ,   3.99525975 ,  51.39600836 ,  73.98036484 , -54.17796862 ,  21.61591299 , -12.43479129 ,  -3.54078479 ,  -5.66893607 ,  43.47934744 , -47.22248303 ,  58.00643945 ,-142.24547841 ,  75.4571528  ,  30.84068789 ,  48.82509585 , -63.79548858 , -12.0651105  ,  -5.94995138 ,  26.46394656 ,  21.61376516 ,  -5.6684142 ,  45.56536849 ,  -0.44191492 ,  51.7966019  , -71.45755854 ,  64.80971002 ,  39.54059307 ,  46.97318198 , -53.47229276 ,  16.13392281 ,   0.33558013 ,  46.16207529 ,  33.71502413 ,  -5.6684142  ,  45.59435143 ,  22.44114991 ,  48.336994   , -19.12355243 ,  55.9343373  ,  42.60482153 ,  45.94498624 , -16.66883355 ,  30.60821494 ,  -0.00972615 ,   0.72039779 , -16.94477838 ,   1.26481702 ,   0.0464093  ,  -0.65534518 ,   0.00675854 ,   0.7203978 ,  12.50531302 ,  -0.12152569 ,  -0.00027075 ,   0.17945797 ,  90.85927153 ,   2.13092033 ,  -1.22866057 , -57.09407799 ,   8.47001552 ,  -0.00176077 ,   3.40268353 ,  -0.07203281 ,  -1.95184606 ,  -0.10104856 ,   0.00181168 ,   0.04610832 ,  -9.30557176 ,   0.06615403 ,   7.41262044 ,  -0.02141117 ,   0.00021472 ,  -0.03124094]\n",
    "    wk0=np.array(wk0);\n",
    "    phiT=np.transpose(phi);\n",
    "    phiTphi=np.dot(phiT,phi);   \n",
    "    phiTy=np.dot(phiT,y);   \n",
    "    alphaBym=alpha;\n",
    "    xaxis=list();\n",
    "    yaxis=list();\n",
    "    #algFixedIteration=True;\n",
    "    logReading=False;\n",
    "    diff=0;\n",
    "    #-----------------------------------------------------------------\n",
    "    #Best Tested Constant\n",
    "    #1)With 13 features i.e original Phi\n",
    "    #normalize phi: true.\n",
    "    #aplha=.212 lamda=.301 tds=300 vds=120 trained o/p=4.8310 rms\n",
    "    #Note: Tried for different initial wk0 from phi matrix but o/p remain same\n",
    "    #\n",
    "    #2)With 26 features i.e newphi=phi + phi^2\n",
    "    #normalize phi: true; algFixedIteration=True; maxIteration=1000000;\n",
    "    #aplha=.0003 lamda=.301 tds=300 vds=120 trained o/p=3.940800315632070 rms\n",
    "    #Tried for different initial wk0 but o/p remain same\n",
    "    #-----------------------------------------------------------------\n",
    "    print(\"Training Started (Least Sq. With Ridge) ...\");\n",
    "    oldRms=0;\n",
    "    currentAlpha=alphaBym;\n",
    "    if (algFixedIteration):\n",
    "        for iteration in range(0,maxIteration):  \n",
    "            wk1=wk0-(currentAlpha*((np.dot(phiTphi,wk0)-phiTy)+(lam*wk0)));              \n",
    "            ystar=pridict(phi,wk1);\n",
    "            rms=getRMS(y,ystar);    \n",
    "            xaxis.append(iteration);\n",
    "            yaxis.append(rms);\n",
    "            percentComplete=((iteration+1)*100)/maxIteration;\n",
    "            if(percentComplete%10==0 ):\n",
    "                print(\"#alpha\",currentAlpha,\"Percent Completed\",percentComplete,\"rms:\",rms);                \n",
    "            wk0=wk1;    \n",
    "            oldRms=rms;\n",
    "    else:\n",
    "        diffOffset=1e-12;\n",
    "        iteration=0;\n",
    "        oldRms=0;\n",
    "        voldRms=0;\n",
    "        while (True):\n",
    "            wk1=wk0-(alphaBym*((np.dot(phiTphi,wk0)-phiTy)+(lam*wk0)));                     \n",
    "            ystar=pridict(phi,wk1);\n",
    "            rms=getRMS(y,ystar);    \n",
    "            xaxis.append(iteration);\n",
    "            yaxis.append(rms);\n",
    "            diff=abs(oldRms-rms);            \n",
    "            if(iteration>0 and diff<diffOffset):\n",
    "                break;\n",
    "            if(False and iteration%100==0 ):\n",
    "                print(\"# iteration: \",iteration,\" rms:\",rms,\"diff:\",diff);            \n",
    "            if(iteration%100000==0):\n",
    "                print(wk1);\n",
    "            wk0=wk1;\n",
    "            oldRms=rms;\n",
    "            iteration+=1;\n",
    "        print(\"# iteration: \",iteration,\" rms:\",rms,\"diff:\",diff);    \n",
    "           \n",
    "    print(\"Final Trained RMS:\",rms ,\". Iteration needed \", iteration);    \n",
    "    #-------------------------------------------------------------\n",
    "    if(logReading):\n",
    "        writeReadingInFile(\"ridge1.csv\",alpha,lam,iteration,rms,2);\n",
    "        plotGraph(xaxis,yaxis);\n",
    "    return wk1;\n",
    "\n",
    "\n",
    "# GD: Least Sq. With ||w||_(1.5)^(1.5)\n",
    "def gardientDescentWithPnom(phi,y,p,lam,alpha,maxIteration=100000,algFixedIteration=True):\n",
    "    m=len(y);#no of data points\n",
    "    n=len(phi[0]);# no. of features    \n",
    "    #alpha=0.0003 #learning parameter\n",
    "    #maxIteration=1000000;\n",
    "    phi=np.array(phi);\n",
    "    y=(np.array(y));#converting row vector to col vector    \n",
    "    wk0=np.zeros(n);# Nx1 vector    \n",
    "    #wk0=phi[1];\n",
    "    #Taken from L2 norm values\n",
    "    #5. Iteration completed: approx 10^11 Note: closed in between as taking too much time       \n",
    "    #No of Features: 122. #9\n",
    "    wk0=[42.8612625  ,  -1.14046757 ,  -1.06252978 ,   0.46944066 ,  -5.60391846 , -70.3726374  ,  80.76000485 ,   1.36490995 , -21.6970572  ,   8.78716327 ,   0.55415016 , -10.34289432 ,   0.31692929 ,  -7.5507631  ,   0.00550376 ,   0.00257729 ,  -0.01818881 ,  -5.63320282 ,  28.32398072 ,  -0.61417676 ,  -0.00260213 ,   0.64126723 ,  -0.09233672 ,  -0.00019618 ,   0.26894577 ,  -0.00044879 ,   0.05783857 ,  -1.72766649 ,  25.18052798 ,  10.13240115 ,  -5.66807777 ,  -3.16682888 ,-294.52215742 , -34.4466717  , 158.72466719 , -71.68585093 , -17.36430966 ,  48.96856689 , -12.49998749 ,  12.72021565 ,  -6.23757794 , -74.29890105 , -43.93825806 ,  -5.66794087 ,  32.83971965 ,-141.54238961 ,  61.41514829 ,-159.23631207 ,  65.81724008 ,   3.99525975 ,  51.39600836 ,  73.98036484 , -54.17796862 ,  21.61591299 , -12.43479129 ,  -3.54078479 ,  -5.66893607 ,  43.47934744 , -47.22248303 ,  58.00643945 ,-142.24547841 ,  75.4571528  ,  30.84068789 ,  48.82509585 , -63.79548858 , -12.0651105  ,  -5.94995138 ,  26.46394656 ,  21.61376516 ,  -5.6684142 ,  45.56536849 ,  -0.44191492 ,  51.7966019  , -71.45755854 ,  64.80971002 ,  39.54059307 ,  46.97318198 , -53.47229276 ,  16.13392281 ,   0.33558013 ,  46.16207529 ,  33.71502413 ,  -5.6684142  ,  45.59435143 ,  22.44114991 ,  48.336994   , -19.12355243 ,  55.9343373  ,  42.60482153 ,  45.94498624 , -16.66883355 ,  30.60821494 ,  -0.00972615 ,   0.72039779 , -16.94477838 ,   1.26481702 ,   0.0464093  ,  -0.65534518 ,   0.00675854 ,   0.7203978 ,  12.50531302 ,  -0.12152569 ,  -0.00027075 ,   0.17945797 ,  90.85927153 ,   2.13092033 ,  -1.22866057 , -57.09407799 ,   8.47001552 ,  -0.00176077 ,   3.40268353 ,  -0.07203281 ,  -1.95184606 ,  -0.10104856 ,   0.00181168 ,   0.04610832 ,  -9.30557176 ,   0.06615403 ,   7.41262044 ,  -0.02141117 ,   0.00021472 ,  -0.03124094]\n",
    "    wk0=np.array(wk0);    \n",
    "    phiT=np.transpose(phi);\n",
    "    phiTphi=np.dot(phiT,phi);   \n",
    "    phiTy=np.dot(phiT,y);   \n",
    "    alphaBym=alpha/m;\n",
    "    #lam=0.31;\n",
    "    xaxis=list();\n",
    "    yaxis=list();\n",
    "    #algFixedIteration=True;\n",
    "    logReading=False;\n",
    "    diff=0;\n",
    "    wPow=p-1;\n",
    "    if (p<=1):\n",
    "        print(\"Error: norm p is less than 1 i.p p=\",wPow);\n",
    "        return None;\n",
    "        \n",
    "    #-----------------------------------------------------------------\n",
    "    print(\"Training Started (Least Sq. With Ridge) ...\");\n",
    "    if (algFixedIteration):\n",
    "        for iteration in range(0,maxIteration):\n",
    "            if (wPow>1):\n",
    "                wk0Pow=np.power(wk0,wPow);            \n",
    "            else:\n",
    "                wk0Pow=wk0;\n",
    "            wk1=wk0-(alphaBym*((np.dot(phiTphi,wk0)-phiTy)+(lam*wk0Pow))); \n",
    "            \n",
    "            ystar=pridict(phi,wk1);\n",
    "            rms=getRMS(y,ystar);    \n",
    "            xaxis.append(iteration);\n",
    "            yaxis.append(rms);\n",
    "            percentComplete=((iteration+1)*100)/maxIteration;\n",
    "            if( percentComplete%10==0 ):\n",
    "                print(\"Percent Completed\",percentComplete,\" rms:\",rms);\n",
    "            wk0=wk1;\n",
    "    else:\n",
    "        diffOffset=1e-20;\n",
    "        iteration=0;\n",
    "        oldRms=0;\n",
    "        voldRms=0;\n",
    "        while (True):            \n",
    "            if (wPow>1):\n",
    "                wk0Pow=np.power(wk0,wPow);            \n",
    "            else:\n",
    "                wk0Pow=wk0;\n",
    "            wk1=wk0-(alphaBym*((np.dot(phiTphi,wk0)-phiTy)+(lam*wk0Pow)));  \n",
    "            ystar=pridict(phi,wk1);\n",
    "            rms=getRMS(y,ystar);    \n",
    "            xaxis.append(iteration);\n",
    "            yaxis.append(rms);\n",
    "            diff=abs(oldRms-rms);                      \n",
    "            \n",
    "            if(iteration>0 and  diff<=diffOffset):\n",
    "                break;\n",
    "            if(False and iteration%100==0 ):\n",
    "                print(\"# iteration: \",iteration,\" rms:\",rms,\"diff:\",diff);            \n",
    "            wk0=wk1;\n",
    "            oldRms=rms;            \n",
    "            iteration+=1;\n",
    "        print(\"# iteration: \",iteration,\" rms:\",rms,\"diff:\",diff);    \n",
    "\n",
    "    print(\"Final Trained RMS:\",rms ,\". Iteration needed \", iteration);       \n",
    "    #-------------------------------------------------------------\n",
    "    if(logReading):\n",
    "        writeReadingInFile(\"pnom.csv\",alpha,lam,iteration,rms,p);\n",
    "        plotGraph(xaxis,yaxis);\n",
    "    return wk1; \n",
    "\n",
    "# Finding w*=(QTQ+lamI)^-1QTY\n",
    "def trainUsingClosedFormRidgeEq(dataset,output,lam=0.0001):\n",
    "    #-------------------------------------\n",
    "    # 1)Best value: m=300 validate=120\n",
    "    # addW0=True: c=0.376654. Not lamd\n",
    "    # Note: Please make eq as phiT_phi+c\n",
    "    #\n",
    "    # 2)Best value: m=300 validate=120\n",
    "    # addW0=True: lam=0.521. Not C. \n",
    "    # trained RMS:  3.938231279881311\n",
    "    # Note: Please make eq as phiT_phi+lamI\n",
    "    #-------------------------------------\n",
    "    m=len(dataset);\n",
    "    n=len(dataset[0]);    \n",
    "    phi=np.array(dataset);\n",
    "    y=np.array(output);\n",
    "    phiT=np.transpose(phi);    \n",
    "    #(QTQ)    \n",
    "    phiT_phi=np.dot(phiT,phi);\n",
    "    n=len(phiT_phi);    \n",
    "    c=.301;\n",
    "    I=np.identity(n);\n",
    "    lamI=lam*I;\n",
    "    d=getDet(phiT_phi)\n",
    "    #--------------------------------------\n",
    "    if(True or d>0):\n",
    "        #(QTQ+lamI)^-1\n",
    "        phiT_phi_inv=inv((phiT_phi+lamI));\n",
    "        #(QTQ+lamI)^-1QT\n",
    "        phiT_phi_inv_phiT=np.dot(phiT_phi_inv,phiT);  \n",
    "        #(QTQ+lamI)^-1QT*Y\n",
    "        w=np.dot(phiT_phi_inv_phiT,y);\n",
    "        return w;\n",
    "    else:\n",
    "        print(\"Error:Phi is NOT full column rank.\");\n",
    "        return None;\n",
    "    pass;\n",
    "\n",
    "\n",
    "\n",
    "def getKernalFeature(phi,wrtdataPoint):    \n",
    "    p=np.array(phi);           \n",
    "    wrtdata=np.array(phiSet[wrtdataPoint]);     \n",
    "    tempMatrix=p-wrtdata;    \n",
    "    fcol=np.sum(tempMatrix,axis=1);\n",
    "    sigma=2;\n",
    "    fcol=np.power(fcol,2);\n",
    "    fcol=fcol*(-1*(1/sigma));\n",
    "    fcol=np.exp(fcol);    \n",
    "    return fcol;\n",
    "    pass;\n",
    "\n",
    "\n",
    "def createNewFeatureMatrix(phi):    \n",
    "    newPhi=np.array(phi);\n",
    "    p=np.array(phi);        \n",
    "    if(addW0Col):#deleting first W0Col coffecient is 1\n",
    "        p=np.delete(p,0,1);    \n",
    "\n",
    "    TotalColumn=len(p[0]);\n",
    "    HalfColumn=int(len(p[0])/2)+1;  \n",
    "    \n",
    "    #---------------------------------------------------------------------------------\n",
    "    #Best Features till now.\n",
    "    #1. feature sq2 with lamda 0.001\n",
    "    #2. feature sq2 + sq root with lamda 0.001\n",
    "    #3. feature sq2 + 1/2 root + 1/4 root with lamda 0.001 rms: 3.02849\n",
    "    #\n",
    "    #4. feature sq2 + 1/2 root + 1/4 root + 1/8 + 1/16  + 1/32 lamda=0.0001\n",
    "    #\n",
    "    #5. feature sq2 + 1/2 root + 1/4 root + 1/8 + 1/16 +  + 1/32 [x0.x1 x0.x2 x0.x3....x0.x7] lamda=0.0001\n",
    "    #\n",
    "    #6. feature sq2 + 1/2 root + 1/4 root + 1/8 + 1/16 +  + 1/32 [x0.x1 x0.x2 ....x0.x7]+ [x0.x1^2 x0.x2^2 ..x0.x7^2] lamda=0.0001\n",
    "    #   train rms= 2.732538945291264 validate Rms=4.711844963854281 test RMS=3.014\n",
    "    #\n",
    "    #7. feature sq2 + 1/2 root + 1/4 root + 1/8 + 1/16 +  + 1/32 [x0.x1 x0.x2 ....] +[x0.x1^2 x0.x2^2...x0.x7^2] \n",
    "    #   [x3.x4^0.05 x3.x5^0.05]\n",
    "    #   lamda=0.0001 train rms= 2.6523470519008407 validate Rms=4.490001110608855 test RMS=2.85636\\\n",
    "    #\n",
    "    #8. feature sq2 + 1/2 root + 1/4 root + 1/8 + 1/16 +  + 1/32 [x0.x1 x0.x1 ....x0.x7] +[x0.x1^2 x0.x2^2..x0.x7^2] \n",
    "    #   + [x3.x4^0.5 x3.x5^0.5 .... x3.x7^0.5]\n",
    "    #\n",
    "    #9. feature sq2 + 1/2 root + 1/4 root + 1/8 + 1/16 +  + 1/32 [x0.x1 x0.x1 ....x0.x7] +[x0.x1^2 x0.x2^2..x0.x7^2] \n",
    "    #   + [x3.x4^0.5 x3.x5^0.5...x3.x7^0.5] + [x3.x4^2.x5^0.4 x3.x4^2.x6^0.4 x3.x4^2.x7^0.4]     \n",
    "    #   lamda=0.0001 train rms=2.6147993385918764 validate Rms=2.6147993385918764 test rms=2.7\n",
    "    #\n",
    "    #10. feature sq2 + 1/2 root + 1/4 root + 1/8 + 1/16 + 1/32 [x0.x1 x0.x1 ...x0.x7] +[x0.x1^2 x0.x2^2 ...x0.x7^2] \n",
    "    #   + [x3.x4^0.5 x3.x5^0.5 ... x3.x7^0.5] + [x3.x4^2.x5^0.4 x3.x4^2.x6^0.4 x3.x4^2.x7^0.4]     \n",
    "    #   + [x7.x10 .... x7.x13]\n",
    "    #   lamda=0.0001 train rms=2.5057404497731555 validate Rms=4.230169910269796 test rms=2.55676    \n",
    "    #\n",
    "    #11. feature sq2 + 1/2 root + 1/4 root + 1/8 + 1/16 + 1/32 [x0.x1 x0.x1 ...x0.x7] +[x0.x1^2 x0.x2^2 ...x0.x7^2] \n",
    "    #   + [x3.x4^0.5 x3.x5^0.5 ... x3.x7^0.5] + [x3.x4^2.x5^0.4 x3.x4^2.x6^0.4 x3.x4^2.x7^0.4]     \n",
    "    #   + [x7.x10 .... x7.x13] + [x7.x10^2 .... x7.x13^2] + [x7.x10^0.5 .... x7.x13^0.5]\n",
    "    #   lamda=0.0001 train rms=2.340500337718980 validate Rms=3.8389206328449315 test rms=2.35368    \n",
    "    # \n",
    "    #--------------------------------------------------------------------------------------------\n",
    "   \n",
    "    absP=np.abs(p)\n",
    "    pPowI=np.power(p,2);        \n",
    "    newPhi=np.concatenate((newPhi,pPowI),axis=1);     \n",
    "        \n",
    "    pPowI=np.power(absP,0.5);        \n",
    "    newPhi=np.concatenate((newPhi,pPowI),axis=1); \n",
    "        \n",
    "    pPowI=np.power(absP,0.25);        \n",
    "    newPhi=np.concatenate((newPhi,pPowI),axis=1); \n",
    "   \n",
    "    pPowI=np.power(absP,0.125);        \n",
    "    newPhi=np.concatenate((newPhi,pPowI),axis=1); \n",
    "       \n",
    "    pPowI=np.power(absP,0.0625);        \n",
    "    newPhi=np.concatenate((newPhi,pPowI),axis=1); \n",
    "        \n",
    "    pPowI=np.power(absP,0.03125);        \n",
    "    newPhi=np.concatenate((newPhi,pPowI),axis=1); \n",
    " \n",
    "    c1=p[:,0];        \n",
    "    for i in range(1,HalfColumn):\n",
    "        c2=p[:,i+1];\n",
    "        c3=c1*c2;\n",
    "        newPhi=np.insert(newPhi,len(newPhi[0]),c3,axis=1);    \n",
    "    \n",
    "    \n",
    "    c1=p[:,0];            \n",
    "    pPowI=np.power(p,2);                \n",
    "    for i in range(1,HalfColumn):\n",
    "        c2=pPowI[:,i+1];\n",
    "        c3=c1*c2;\n",
    "        newPhi=np.insert(newPhi,len(newPhi[0]),c3,axis=1);    \n",
    "    \n",
    "    \n",
    "    c1=p[:,3];                \n",
    "    pPowI=np.power(absP,0.5);                \n",
    "    for i in range(4,HalfColumn):\n",
    "        c2=pPowI[:,i];\n",
    "        c3=c1*c2;\n",
    "        newPhi=np.insert(newPhi,len(newPhi[0]),c3,axis=1);    \n",
    "    \n",
    "    c1=p[:,3];      \n",
    "    c2=p[:,4]**2;\n",
    "    n=int(len(p[0])/2)+1;                    \n",
    "    pPowI=np.power(absP,0.4);                \n",
    "    for i in range(5,HalfColumn):\n",
    "        c3=pPowI[:,i];\n",
    "        c=c1*c2*c3;\n",
    "        newPhi=np.insert(newPhi,len(newPhi[0]),c,axis=1);    \n",
    "    \n",
    "    n=int(len(p[0])/2)+1;                 \n",
    "    c1=p[:,7];      \n",
    "    for i in range(9,TotalColumn):\n",
    "        c3=p[:,i];\n",
    "        c=c3*c1;\n",
    "        newPhi=np.insert(newPhi,len(newPhi[0]),c,axis=1);    \n",
    "       \n",
    "    c1=np.abs(p[:,7])**2;\n",
    "    for i in range(10,TotalColumn):\n",
    "        c3=p[:,i];\n",
    "        c=c3*c1;\n",
    "        newPhi=np.insert(newPhi,len(newPhi[0]),c,axis=1);    \n",
    "    \n",
    "    c1=np.abs(p[:,7])**0.5;\n",
    "    for i in range(10,TotalColumn):\n",
    "        c3=p[:,i];\n",
    "        c=c3*c1;\n",
    "        newPhi=np.insert(newPhi,len(newPhi[0]),c,axis=1);    \n",
    "    \n",
    "    c1=np.abs(p[:,7])**0.5;\n",
    "    pPowI=np.power(absP,2);                   \n",
    "    for i in range(10,TotalColumn):\n",
    "        c3=pPowI[:,i];\n",
    "        c=c3*c1;\n",
    "        newPhi=np.insert(newPhi,len(newPhi[0]),c,axis=1);         \n",
    "    \n",
    "    return newPhi;    \n",
    "    pass;\n",
    "\n",
    "#--settings--\n",
    "np.set_printoptions(suppress=True)\n",
    "#---init---\n",
    "dir=\"\"\n",
    "trainFile=dir+\"train.csv\";\n",
    "testFile=dir+\"test.csv\";\n",
    "trainDSSizePercentage=0.7; # x*100 percentage. 1-x data set will be used for validating\n",
    "addW0Col=True;\n",
    "tdSize=280;\n",
    "#---------------------------------------------\n",
    "print(\"Fetching Trained Dataset from file...\");\n",
    "dataset=readTrainData(trainFile);\n",
    "testDS=readTestData(testFile);\n",
    "phiSet=dataset[0];\n",
    "ySet=dataset[1];\n",
    "\n",
    "newPhiSet=createNewFeatureMatrix(phiSet);\n",
    "newTestDS=createNewFeatureMatrix(testDS);\n",
    "\n",
    "\n",
    "phiSet_norm=normalizePhi(newPhiSet);\n",
    "testDS_norm=normalizePhi(newTestDS);\n",
    "\n",
    "tds=spitTrainDataset(phiSet,ySet,tdSize);\n",
    "tds_norm=spitTrainDataset(phiSet_norm,ySet,tdSize);\n",
    "\n",
    "print(\"Fetching of data Completed.\");\n",
    "\n",
    "#train set\n",
    "trainDatasetPhi=tds[0];\n",
    "trainDatasetY=tds[1];\n",
    "validateDatasetPhi=tds[2];\n",
    "validateDatasetY=tds[3];\n",
    "gtdPhi=trainDatasetPhi;\n",
    "\n",
    "trainDatasetPhi_norm=tds_norm[0];\n",
    "trainDatasetY_norm=tds_norm[1];\n",
    "validateDatasetPhi_norm=tds_norm[2];\n",
    "validateDatasetY_norm=tds_norm[3];\n",
    "\n",
    "\n",
    "trainDatasetPhi=createNewFeatureMatrix(trainDatasetPhi);\n",
    "validateDatasetPhi=createNewFeatureMatrix(validateDatasetPhi);\n",
    "testDS=createNewFeatureMatrix(testDS);\n",
    "\n",
    "print(\"Train Size:\"+str(len(trainDatasetPhi)));\n",
    "print(\"Validate Size:\"+str(len(validateDatasetPhi)));\n",
    "\n",
    "ds=[trainDatasetPhi,trainDatasetY,validateDatasetPhi,validateDatasetY,testDS];\n",
    "ds_norm=[trainDatasetPhi_norm,trainDatasetY,validateDatasetPhi_norm,validateDatasetY,testDS_norm];\n",
    "#mainClosedFormSol(ds);\n",
    "#mainGradientDesent(ds_norm,lamdaVal=0.0001,alphaVal=0.000000000000001,maxIter=1000000);\n",
    "#mainGradientDesentLpnorm(ds_norm,3/2,lamdaVal=0.0001,alphaVal=0.00000000001,maxIter=100000);\n",
    "\n",
    "mainRidgeClosedFormSol(ds,0.0001);\n",
    "#mainGradientDesent(ds,lamdaVal=0.0001,alphaVal=0.000000000000001,maxIter=100000);\n",
    "#mainGradientDesentLpnorm(ds,3/2,\"output_p1\",lamdaVal=0.0001,alphaVal=0.00000000001,maxIter=100000);\n",
    "#mainGradientDesentLpnorm(ds,5/4,\"output_p2\",lamdaVal=0.0001,alphaVal=0.00000000001,maxIter=100000);\n",
    "#mainGradientDesentLpnorm(ds,7/6,\"output_p3\",lamdaVal=0.0001,alphaVal=0.00000000001,maxIter=100000);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
